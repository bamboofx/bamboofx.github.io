<!DOCTYPE html>
<html lang="en" class="js csstransforms3d">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="generator" content="Hugo 0.57.2" />
    <meta name="description" content="">


    <link rel="icon" href="/images/favicon.png" type="image/png">

    <title>Chapter 2 :: Bamboofx&#39;s or NTVFX&#39;s Site</title>

    
    <link href="/css/nucleus.css?1568110658" rel="stylesheet">
    <link href="/css/fontawesome-all.min.css?1568110658" rel="stylesheet">
    <link href="/css/hybrid.css?1568110658" rel="stylesheet">
    <link href="/css/featherlight.min.css?1568110658" rel="stylesheet">
    <link href="/css/perfect-scrollbar.min.css?1568110658" rel="stylesheet">
    <link href="/css/auto-complete.css?1568110658" rel="stylesheet">
    <link href="/css/atom-one-dark-reasonable.css?1568110658" rel="stylesheet">
    <link href="/css/theme.css?1568110658" rel="stylesheet">
    <link href="/css/hugo-theme.css?1568110658" rel="stylesheet">
    
      <link href="/css/theme-blue.css?1568110658" rel="stylesheet">
    

    <script src="/js/jquery-3.3.1.min.js?1568110658"></script>

    <style>
      :root #header + #content > #left > #rlblock_left{
          display:none !important;
      }
      
    </style>
    
  </head>
  <body class="" data-url="/ai-with-python/chapter2/">
    <nav id="sidebar" class="">



  <div id="header-wrapper">
    <div id="header">
      <a id="logo" href="https://bamboofx.github.io/">
  <img src="https://bamboofx.github.io/images/logo.png "> </img>
</a>

    </div>
    
        <div class="searchbox">
    <label for="search-by"><i class="fas fa-search"></i></label>
    <input data-search-input id="search-by" type="search" placeholder="Search...">
    <span data-search-clear=""><i class="fas fa-times"></i></span>
</div>

<script type="text/javascript" src="/js/lunr.min.js?1568110658"></script>
<script type="text/javascript" src="/js/auto-complete.js?1568110658"></script>
<script type="text/javascript">
    
        var baseurl = "https:\/\/bamboofx.github.io\/";
    
</script>
<script type="text/javascript" src="/js/search.js?1568110658"></script>

    
  </div>

    <div class="highlightable">
    <ul class="topics">

        
          
          


 
  
    
    <li data-nav-id="/ai-with-python/" title="AI with Python (Prateek Joshi)" class="dd-item 
        parent
        
        
        ">
      <a href="/ai-with-python/">
          AI with Python (Prateek Joshi)
          
      </a>
      
      
        <ul>
          
          
          
          
        
          
            
            


 
  
    
      <li data-nav-id="/ai-with-python/chapter1/" title="Chapter 1" class="dd-item ">
        <a href="/ai-with-python/chapter1/">
        Chapter 1
        
        </a>
    </li>
     
  
 

            
          
            
            


 
  
    
      <li data-nav-id="/ai-with-python/chapter2/" title="Chapter 2" class="dd-item active">
        <a href="/ai-with-python/chapter2/">
        Chapter 2
        
        </a>
    </li>
     
  
 

            
          
        
        </ul>
              
    </li>
  
 

          
          


 
  
    
    <li data-nav-id="/about/" title="about" class="dd-item 
        
        
        
        ">
      <a href="/about/">
          About
          
      </a>
      
              
    </li>
  
 

          
         
    </ul>

    
    

    
    <section id="footer">
      <p>Built with <a href="https://github.com/matcornic/hugo-theme-learn"><i class="fas fa-heart"></i></a> from <a href="https://getgrav.org">Grav</a> and <a href="https://gohugo.io/">Hugo</a></p>

    </section>
  </div>
</nav>





        <section id="body">
        <div id="overlay"></div>
        <div class="padding highlightable">
              
              <div>
                <div id="top-bar">
                
                
                <div id="breadcrumbs" itemscope="" itemtype="http://data-vocabulary.org/Breadcrumb">
                    <span id="sidebar-toggle-span">
                        <a href="#" id="sidebar-toggle" data-sidebar-toggle="">
                          <i class="fas fa-bars"></i>
                        </a>
                    </span>
                  
                  <span id="toc-menu"><i class="fas fa-list-alt"></i></span>
                  
                  <span class="links">
                 
                 
                    
          
          
            
            
          
          
            
            
          
          
            <a href='/'>Site</a> > <a href='/ai-with-python/'>AI with Python (Prateek Joshi)</a> > Chapter 2
          
         
          
         
          
        
                 
                  </span>
                </div>
                
                    <div class="progress">
    <div class="wrapper">

    </div>
</div>

                
              </div>
            </div>
            
        <div id="head-tags">
        
        </div>
        
        <div id="body-inner">
          
            <h1>
              
              Chapter 2
            </h1>
          

        



<html><head><meta http-equiv="Content-Type" content="text/html;charset=utf-8" /><link href="style.css" rel="stylesheet" type="text/css" /><title>PythonAI chapter2</title></head><body><div class="calibre" id="calibre_link-0">
	<p class="block_1">Classification and Regression </p>
	<p class="block_2">Using Spervisised Learning</p>
	<p class="block_3">Trong chương này , chúng ta sẽ học về phân loại và quy hợp của dữ liệu sử dụng kỹ thuật học giám sát. </p>
	<p class="block_3">Học hết chương này bạn sẽ biết về các chủ đề sau:</p>
	<ul class="list_">
	<li class="block_4">Sự khác nhau giữa học giám sát và học không giám sát?</li>
	<li class="block_5">Hồi quy là cái gì?</li>
	<li class="block_5">Làm sao để sử lý thô dữ liệu sử dụng những thuật toán.</li>
	<li class="block_5">Cái gì là mã hóa nhãn hiệu</li>
	<li class="block_5">Làm sao để xây dựng một Cơ sở để phân loại hồi quy.</li>
	<li class="block_5"><span class="calibre1">Phân loại Naivi Bayes   là cái gì.</span></li>
	<li class="block_5">Ma trận hỗn loạn là cái gì?</li>
	<li class="block_5">Support Vector Machines là cái gì và cách xây dựng một bộ phân loại dựa trên nó ?</li>
	<li class="block_5">Tuyến tính và quy hợp đa thức là cái gì?</li>
	<li class="block_5">Làm sao để xây dựng một hồi quy tuyến tính cho biến đơn và dữ liệu đa biến ?</li>
	<li class="block_6">Làm sao để ước tính giá nhà ở sử dụng Support Vector Regressor.</li>
</ul>
	<p class="block_7">Giám sát và không giám sát học tập</p>
	<p class="block_8">Machine Learning (ML) máy học thường được chia các <span class="calibre1">làm  nhóm</span> :</p>
	<ul class="list_">
	<li class="block_4">Học Có giám Sát ( Supervised learning): <span class="tab">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span></li>
</ul>
	<ul class="list_1">
	<li class="block_9">Học bán giám sát ( semi-supervised learning)</li>
</ul>
	<ul class="list_">
	<li class="block_5">Học Không giám sát (UnSupervised learning)</li>
	<li class="block_5">Học củng cố (Reingorcement Learning)</li>
</ul>
	<p class="block_10">&nbsp;</p>
	<p class="block_8">Một trong những cách thông dụng để truyền trí thông minh nhân tạo vào một cái máy là thông qua việc cho máy tự học (machine learning). Thế giới của máy tính tự học được phân chia rộng rãi cho học giám sát và không giám sát. Cũng có vài cách chia khác như bán giám sát và củng cố…, nhưng chúng ta sẽ thảo luận về nó sau.</p>
	<p class="block_8"><b class="calibre2">Học Có Giám Sát (Supervised learning) </b> Là đề cập đến quá trình xây dựng một mô hình máy tính tự học dựa trên những dữ liệu đã train được dán nhãn sẵn. </p>
	<p class="block_8">Đây là thuật toán dự đoán đầu ra (output) của một dữ liệu mới (new input) dựa trên các cặp (input,output) đã biết trước. Cặp dữ liệu này còn được gọi là [dữ liệu,nhãn] (data, label) [X,y]</p>
	<p class="block_8">Ta có một mảng (array) biến đầu vào X=[x1,x2,..xn] và một mảng nhãn tương ứng Y=[y1,y2,…,yn] các cặp dữ liệu biết trước  được gọi là dữ liệu huấn luyện (training data) . Từ dữ liệu này chúng ta cần tạo ra một hàm số để ánh xạ (map) mỗi phần tử (item) từ array X sang một phần tử (item) tương ứng của array Y.  . Mục đích của việc này làm giá trị của hàm số  gần đúng nhất để mỗi khi có một dữ liệu <span class="calibre1"> mới chúng ta có thể tính được nhãn (label) tương ứng của nó            </span></p>
	<p class="block_8"><b class="calibre2">ví dụ : </b>Trong nhận dạng khuân mặt. ta có một mảng dữ liệu gồm nhiều ảnh của nhiều người khác nhau mỗi người lại có vài tấm ảnh chụp ở những vị trí khác nhau. Chúng ta đưa những bức ảnh này vào trong một thuật toán và chỉ cho nó biết mỗi bức ảnh tương ứng với tên là gì gì. Sau đó thuật toán sẽ tạo ra một mô hình (model) tức là một hàm số có đầu vào (x) là một bức ảnh và đầu ra (y) là một cái tên. Khi ta đưa vào một bức ảnh mà mô hình (model) đó chưa thấy bao giờ nó sẽ dự đoán (predict) bức ảnh đó là ai.</p>
	<p class="block_8"><b class="calibre2">ví dụ khác: </b>Chúng ta muốn xây dựng một hệ thống dự đoán thu nhập của một người, dựa trên những dữ liệu khác nhau như độ tuổi, trình độ học vấn, vị trí …vv..v. Để làm điều đó chúng ta cần tạo ra một cơ sở dữ liệu của người đó thật chi tiết và dán nhãn cho nó. Làm được điều này chúng ta đã nói với thuật toán của chúng ta cần tham số gì để hiểu thu nhập tương ứng với cái gì. Dựa vào việc lập biểu đồ ánh xạ (mapping) tương ứng, thuật toán sẽ học làm cách nào để tính toán thu nhập của một người sử dụng những tham số đã được cho trước.</p>
	<p class="block_8"><b class="calibre2">Học không giám sát( unsupervised learning) </b> là thuật toán đề cập đến quá trình xây dựng máy tự học (ML) mà không dựa trên những dữ liệu được dán nhãn, chúng ta sẽ không biết được đầu ra(output) hay nhãn(label) tương ứng cho dữ liệu đầu vào(input). Trong một vài ý nghĩa thì nó là những thứ ngược lại với những thứ ta vừa nói ở trên. Vì không có những nhãn tương ứng đã được biết trước, bạn cần phân tích những dữ liệu đầu vào. Ví dụ, Chúng ta cần xây dựng một hệ thống mà trong đó chúng ta phải tách một tập hợp các dữ liệu thành các nhóm (Clustering datas) khác nhau. Cái khó ở đây là chúng ta không biết chính xác các điều kiện để tách dữ liệu. Vì thế một thuật toán học tập không giám sát cần tách dự liệu đã cho thành một số của các nhóm theo cách tốt nhất có thể.</p>
	<p class="block_8">Phân nhóm dữ liệu (Clustering data): Chúng ta phải phân nhóm toàn bộ dữ liệu đầu vào (input) thành các nhóm dựa trên sự liên quan giữa các dữ liệu đã có trong mỗi nhóm. Vd: Phân nhóm khách hàng dựa vào hành vi mua hàng</p>
	<p class="block_8">Tổng hợp dữ liệu ( Association Data): Là bài toán khi chúng ta cần tìm ra một quy luật dựa trên dữ liệu đầu vào , hoặc dữ liệu cho trước (input). Vd: Những khách hàng nam mua quần <span class="calibre1">áo  thường</span> có xu hướng mua them đồng hồ, bóp hay thắt lưng. Những khách hàng xem phim Avenger thường thích các phim hành động. Dựa vào đó chúng ta tạo ra một hệ thống gợi ý khách hàng ( Recommend System).</p>
	<p class="block_8">Túm lại học không giám sát là khi chúng ta chỉ có dữ liệu đầu vào (input) x mà không biến nhãn y tương ứng cho nó.</p>
	<p class="block_7">Phân loại là gì?</p>
	<p class="block_8">Trong chương này chúng ta sẽ thạo luận về kỹ thuật phân loại không giám sát. Quá trình phân loại là một kỹ thuật trong đó chúng ta phân loại dữ liệu thành các lớp nhất định. Trong quá trình phân loại, Chúng ta xắp xếp dữ liệu vào trong một số cố định của một thể loại xác định để nó có thể sử dụng một cách hiệu quả nhất.</p>
	<p class="block_8">Trong lĩnh vực máy học<span class="calibre1">  </span>(machine learning) ,phân loại (classification) giải quyết vấn đề nhận diện và phân loại danh mục cho một dữ liệu mới. Chúng ta xây dựng một mẫu phân loại ( classification<span class="calibre1"> model) dựa trên việc huấn luyện tập dữ liệu chứa dữ liệu và  những vấn đề nguyên bản được dán nhãn. Ví dụ, chúng ta muốn kiểm tra không biết hình ảnh được nhập có chứa mặt của người hay </span>không . Chúng ta phải xây dựng trạm dữ liệu (dataset) chứa 2 lớp nguyên bản (classes corresponding) chứa: Face và no-face. Sau đó chúng ta phải huấn luyện cho mẫu (model) dựa trên nhũng ví dụ mà chúng ta có, những nguyên mẫu (models) này sau đó được sử dụng để suy luận.</p>
	<p class="block_8">Một hệ thống phân loại tốt là nó có thể dễ dàng tìm và trả lại dữ liệu. Nó được sử dụng rộng rãi trong việc nhận diện khuôn mặt, kiểm tra spam, những máy móc được khuyên dùng vv..vv. Thuật toán phân loại dữ liệu sẽ đưa ra các tiêu chí phù hợp để phân tích dữ liệu được đưa vào và số lượng lớp.</p>
	<p class="block_8">Chúng ta cần chuẩn bị một lượng lớn và đầy đủ các ví dụ để có thể dự đoán. Nếu không đủ các ví dụ thì thuật toán sẽ rất khó trong việc xắp xếp dữ liệu. điều này cũng giống như nó làm việc trên những cơ sở dữ liệu không xác định .</p>
	<p class="block_7">Tiền xử lý dữ liệu( Preprocessing data)</p>
	<p class="block_8">Chúng ta xử lý rất nhiều dữ liệu thô trong thế giới thực. Những thuật toán của máy tự học cần có được dữ liệu đã được định dạng cơ bản trước khi bắt đâu quá trình luyện tập. Để chuẩn bị dữ liệu nhập cho máy tính huấn luyện dữ liệu chúng ta phải xử lý dữ liệu thô và chuyển dổi nó vào định dạng chính xác chúng ta cần. Hãy xem làm thế nào với python.</p>
	<p class="block_8">Tạo một file Python mới và import các package sau:</p>
	<div class="frame_"><p class="block_11">import numpy as np</p><p class="block_12">from sklearn import preprocessing</p></div>
	<p class="block_13">&nbsp;</p>
	<p class="block_14">define sampe data :</p>
	<div class="frame_"><p class="block_11">input_data = np.array([</p><p class="block_15"><i class="calibre3"><span class="calibre4">                       </span></i><i class="calibre3">[5.1, -2.9, 3.3],</i></p><p class="block_15"><i class="calibre3"><span class="calibre4">                       </span></i><i class="calibre3">[-1.2, 7.8, -6.1],</i></p><p class="block_15"><i class="calibre3"><span class="calibre4">                       </span></i><i class="calibre3">[3.9, 0.4, 2.1],</i></p><p class="block_16"><i class="calibre3"><span class="calibre4">                       </span></i><i class="calibre3">[7.3, -9.9, -4.5]])</i></p></div>
	<p class="block_8">Chúng ta cần nói về vài kỹ thuật xử lý dữ liệu thô. Hãy bắt đầu với binaraization:</p>
	<ul class="list_">
	<li class="block_4">Binarization</li>
	<li class="block_5">Mean removal</li>
	<li class="block_5">Scaling</li>
	<li class="block_6">Normalization</li>
</ul>
	<p class="block_7">Binarization</p>
	<p class="block_8">Cách xử lý này được sử <span class="calibre1">dụng  khi</span> chúng ta muốn biến đổi những giá trị số thành giá trị đúng và sai (1,0). Hãy sử dụng hàm có sẵn để nhị phân( binarize ) dữ liệu được nhập vào,ở đây giá trị ngưỡng ( threshold value) bằng 2.1 .</p>
	<p class="block_13"><span class="text_">giải thích 1 chút về giá trị ngưỡng : Ví dụ thang điểm đánh giá học sinh là từ 1 đến 10.&nbsp;Trong một tập hợp gồm 40 học sinh của 1 lớp, người ta muốn phân lọai ra hai miền, miền thứ nhất bao gồm các học sinh đạt yêu cầu và miền thứ hai gồm các học sinh không đạt.&nbsp;Trong tình huống đó người ta dùng giá trị 5 (điểm) như là một ngưỡng (threshold) để phân loại học sinh.</span><span class="text_1"> </span><span class="text_">Giá trị ngưỡng thường được xác định dựa vào những điểm đặc biệt (ví dụ ở trung bình), dựa vào kinh nghiệm khảo sát.</span></p>
	<p class="block_13">&nbsp;</p>
	<p class="block_8">Thêm đoạn code sau vào file Python đã tạo:</p>
	<div class="frame_"><p class="block_11">#Binarize data</p><p class="block_17">data_binarized =</p><p class="block_17">preprocessing.Binarizer(threshold=2.1).transform(input_data)</p><p class="block_12">print("\nBinarized data:\n", data_binarized)</p></div>
	<p class="block_8">Nếu chạy đoạn code bên trên bạn sẽ thấy kết quả được đưa ra như sau:</p>
	<div class="frame_"><p class="block_11">Binarized data:</p><p class="block_17">[[ 1. 0. 1.]</p><p class="block_17">[ 0. 1. 0.]</p><p class="block_17">[ 1. 0. 0.]</p><p class="block_12">[ 1. 0. 0.]]</p></div>
	<p class="block_8">Giống như chúng ta đã thấy , tất cả các giá trị lớn hơn 2.1 sẽ trở thành 1 còn lại sẽ thành 0.</p>
	<p class="block_7">Mean Removal ( Chuẩn hóa dữ liệu - standardization) </p>
	<p class="block_8">Loại bỏ giá trị trung bình (Removing the mean) là phương pháp xử lý sơ cấp thường được sử dụng trong máy tự học. Nó thường hữu dụng trong việc bỏ giá trị trung bình khỏi vector của chúng ta, sao cho mỗi toán tử ở giữa số 0. Chúng ta làm điều này để loại bỏ bias từ vector chính.</p>
	<p class="block_8">Code: </p>
	<div class="frame_"><p class="block_11"># Print mean and standard deviation</p><p class="block_17">print("\nBEFORE:")</p><p class="block_17">print("Mean =", input_data.mean(axis=0))</p><p class="block_12">print("Std deviation =", input_data.std(axis=0))</p></div>
	<p class="block_8">Dòng đầu tiên hiển thị độ lệch trung bình và dòng 2 là lệch chuẩn của dữ liệu đầu vào. </p>
	<p class="block_8">Loại bỏ giá trị trung bình bằng đoạn code sau :</p>
	<div class="frame_"><p class="block_11"># Remove mean</p><p class="block_17">data_scaled = preprocessing.scale(input_data)</p><p class="block_17">print("\nAFTER:")</p><p class="block_18"><i class="calibre3">print("Mean =", data_scaled.mean(axis=0))<span class="calibre4">               </span></i><span class="text_2"># tính giá trị trung bình mỗi cột</span></p><p class="block_19"><i class="calibre3">print("Std deviation =", data_scaled.std(axis=0))<span class="calibre4">      </span></i><span class="text_2"># tính giá trị phương sai mỗi cột</span></p></div>
	<p class="block_8">Nếu chúng ta chạy đoạn code trên thì chúng ta sẽ có kết quả như sau ở Terminal:</p>
	<div class="frame_1"><p class="block_20">BEFORE:</p><p class="block_21">Mean = [ 3.775 -1.15 -1.3 ]</p><p class="block_21">Std deviation = [ 3.12039661 6.36651396 4.0620192 ]</p><p class="block_21">AFTER:</p><p class="block_21">Mean = [ 1.11022302e-16 0.00000000e+00 2.77555756e-17]</p><p class="block_22">Std deviation = [ 1. 1. 1.]</p></div>
	<p class="block_8">Từ những giá trị ta hiển thị ra ta thấy giá trị trung bình rát gần với 0 và độ lệch chuẩn là 1.</p>
	<p class="block_8">Mean và std devation lần lượt là vector kỳ vọng và phương sai của toàn bộ dữ liệu training.</p>
	<p class="block_7">Scaling (tỉ lệ dữ liệu)</p>
	<p class="block_8">Trong vector chính của chúng ta, giá trị của mỗi thuộc tính có thể chứa nhiều giá trị ngẫu nhiên khác nhau. Vì thế điều này sẽ làm ảnh hưởng tới độ chính xác của các thuật toán. Chính vị vậy chúng ta phải tiến hành điều chỉnh dữ liệu để các thuộc tính cùng có chung một tỷ lệ (data scaling). Và thường các thuộc tính có giá trị trong khoảng 0-1.</p>
	<p class="block_8">Thêm đoạn code sau vào file Python bên trên:</p>
	<div class="frame_"><p class="block_11"># Min max scaling</p><p class="block_17">data_scaler_minmax = preprocessing.MinMaxScaler(feature_range=(0, 1))</p><p class="block_17">data_scaled_minmax = data_scaler_minmax.fit_transform(input_data)</p><p class="block_12">print("\nMin max scaled data:\n", data_scaled_minmax)</p></div>
	<p class="block_8">Nếu đoạn code trên được chạy bạn sẽ thấy đoạn sau được in ra trong Terminal:</p>
	<div class="frame_"><p class="block_23">Min max scaled data:</p><p class="block_24"><i class="calibre3">[[ 0.74117647 <span class="calibre4">            </span></i><i class="calibre3">0.39548023 <span class="calibre4">             </span></i><i class="calibre3">1. ]</i></p><p class="block_24"><i class="calibre3">[ 0. <span class="calibre4">                              </span></i><i class="calibre3">1. <span class="calibre4">                               </span></i><i class="calibre3">0. ]</i></p><p class="block_24"><i class="calibre3">[ 0.6 <span class="calibre4">                           </span></i><i class="calibre3">0.5819209 <span class="calibre4">              </span></i><i class="calibre3">0.87234043]</i></p><p class="block_25"><i class="calibre3">[ 1. <span class="calibre4">                             </span></i><i class="calibre3">0. <span class="calibre4">                             </span></i><i class="calibre3">0.17021277]]</i></p></div>
	<p class="block_8">Mỗi hàng được chia tỉ lệ sao cho giá trị tối đa là 1 và tất các các giá trị khác đều có liên quan đến giá trị này.</p>
	<p class="block_7">Normalization (Bình thường(đơn giản) hóa dữ liệu)</p>
	<p class="block_8">Bình thường hóa dữ liệu là điều chỉnh tỉ lệ dữ liệu sao cho mỗi thể hiện đều cho độ dài là 1. Trong Máy Học, chúng ta sử dụng rất nhiều kiểu mẫu đơn giản hóa. Một vài hình thức phổ biến nhất của đơn giản hóa là đặt mục tiêu <span class="calibre1">để  điều</span> chỉnh dữ liệu thành tổng 1. L1 normalization, trong đó đề cập đếnđộ lệch tuyệt đối nhỏ nhất (Least apsolute Deviations<span class="calibre1">)  nó</span> hoạt động theo cách tổng các giá trị tuyệt đối trên mỗi hàng là 1. L2 Normalization , là cách làm bình phương tối thiểu, được làm bằng cách làm cho tổng bình phương thành 1. </p>
	<p class="block_8">Nói chung, L1 Normalization là kỹ thuật được xem làm mạnh mẽ hơn L2 Normalization. L1 normalization được xem là mạnh mẽ hơn vì nó ít có ngoại lệ trong dữ liệu. Rất nhiều lần, dữ liệu thường có xu thế chứa những ngoại lệ và chúng ta không biết làm gì với nó. Chúng ta muốn sử dụng những kỹ thuật an toàn và có ảnh hưởng tốt bỏ qua những ngoại lệ đó trong tính toán. Nếu chúng ta phải giải quyết vấn đề mà phần ngoại lệ là quan trọng, thì khi đó L2 Normalization trở thành lựa chọn tốt hơn.</p>
	<p class="block_8">Thêm dòng sau vào file Python có sẵn :</p>
	<div class="frame_"><p class="block_11"># Normalize data</p><p class="block_17">data_normalized_l1 = preprocessing.normalize(input_data, norm='l1')</p><p class="block_17">data_normalized_l2 = preprocessing.normalize(input_data, norm='l2')</p><p class="block_17">print("\nL1 normalized data:\n", data_normalized_l1)</p><p class="block_12">print("\nL2 normalized data:\n", data_normalized_l2)</p></div>
	<p class="block_8">Terminal output:</p>
	<div class="frame_1"><p class="block_20">L1 normalized data:</p><p class="block_21">[[ 0.45132743 <span class="calibre1">        </span>-0.25663717<span class="calibre1">                </span> 0.2920354 ]</p><p class="block_21">[-0.0794702<span class="calibre1">           </span> 0.51655629 <span class="calibre1">                </span>-0.40397351]</p><p class="block_21">[ 0.609375 <span class="calibre1">             </span>0.0625 <span class="calibre1">                           </span>0.328125 ]</p><p class="block_21">[ 0.33640553 <span class="calibre1">        </span>-0.4562212<span class="calibre1">                  </span> -0.20737327]]</p><p class="block_21">L2 normalized data:</p><p class="block_21">[[ 0.75765788<span class="calibre1">           </span> -0.43082507<span class="calibre1">             </span> 0.49024922]</p><p class="block_21">[-0.12030718 <span class="calibre1">             </span>0.78199664 <span class="calibre1">            </span>-0.61156148]</p><p class="block_21">[ 0.87690281<span class="calibre1">             </span> 0.08993875 <span class="calibre1">             </span>0.47217844]</p><p class="block_22">[ 0.55734935 <span class="calibre1">            </span>-0.75585734 <span class="calibre1">            </span>-0.34357152]]</p></div>
	<p class="block_13">&nbsp;</p>
	<p class="block_13">&nbsp;</p>
	<p class="block_7">Label encoding ( Mã hóa nhãn )</p>
	<p class="block_8">Khi thực hiện phân loại chúng ta thường xử lý rất nhiều nhãn dán. Những nhãn này có thể trong hình thức của từ, số hoặc vài thứ khác. Các functions máy học trong <b class="calibre2">sklearn </b>phân tích chúng dưới dạng số. Vậy nếu chúng ta đã có những số, sau đó chúng ta có thể dùng chúng một cách trực tiếp để bắt đầu training, Nhưng nó không phải cách dùng trong trường hợp này.</p>
	<p class="block_8">Trong thế giới thực, <b class="calibre2">Nhãn dán </b>được biết dưới dạng chữ, bởi chữ là những thứ con người có thể đọc. Chúng ta ghi nhãn những dữ liệu được training bằng chữ để<span class="text_3"> mapping</span> có thể theo dõi được. Để chuyển đổi các nhãn dưới dạng chữ thành số. chúng ta sử dụng một dạng mã hóa <b class="calibre2">Label</b> (label encoder)<b class="calibre2">. </b> Label encoding là quá trình biến đổi <b class="calibre2">labels </b>từ dạng chữ sang dạng số. Nó là cách kích hoạt thuật toán để mổ xẻ dữ liệu của chúng ta:</p>
	<p class="block_8">Tạo một file Python mới và import numpy,sklearn packages:</p>
	<div class="frame_"><p class="block_11">Import numpy as np</p><p class="block_12">Form sklearn import preprocessing</p></div>
	<p class="block_8">Định danh một vài <b class="calibre2">labels:</b></p>
	<div class="frame_"><p class="block_11"># Sample input_labels</p><p class="block_12">input_labels=[‘red’, ‘black’, ‘red’, ‘green’, ‘black’, ‘yellow’, ‘white’]</p></div>
	<p class="block_8">Tạo một mã hóa labels object và luyện tập cho nó:</p>
	<div class="frame_"><p class="block_11"># create label encoder and fit the labels</p><p class="block_17">encoder = preprocessing.LabelEncoder()</p><p class="block_12">encoder.fit(input_labels)</p></div>
	<p class="block_8">In những ánh xạ giữa từ và số:</p>
	<div class="frame_"><p class="block_11"># print the mapping</p><p class="block_17">print(“\nLabel mapping:”)</p><p class="block_17">for i, item in enumerate(encoder.classes_):</p><p class="block_26">print(item,’-’,i)</p></div>
	<p class="block_8">Ta hãy xóa một vài nhãn ngẫu nhiên để xem cách mã hóa hoạt động thế nào:</p>
	<div class="frame_"><p class="block_11"># encode a set ò labels using encoder</p><p class="block_17">test_labels = [‘green’, ‘red’, ‘black’]</p><p class="block_17">encoded_values=encoder.transform(test_labels)</p><p class="block_17">print(“\n Labels=”,test_labels)</p><p class="block_12">print(“Encoded values=”,list(encoded_values))</p></div>
	<p class="block_8">Bây giờ thì giải mã một vài số ngẫu nhiêu:</p>
	<div class="frame_"><p class="block_11"># Decode a set of values using the encoder</p><p class="block_17">encoded_values=[3,0,4,1]</p><p class="block_17">decoded_list= encoder.inverse_transform(encoded_values)</p><p class="block_17">print(“\nEncoded values=”,encoded_values)</p><p class="block_12">print(“\nDecoded labels=”,list(decoded_list))</p></div>
	<p class="block_8">Rồi giờ thì chạy đoạn code trên với pycham. Kết quả sẽ giống thế này:</p>
	<p class="block_27"><span class="text_4">PS D:\</span><span class="text_5">AI Python&gt; &amp; C:/WPy64-3720/python-3.7.2.amd64/python.exe "d:/AI Python/Test.py"</span></p>
	<p class="block_28">&nbsp;</p>
	<p class="block_29">Label mapping:</p>
	<p class="block_27"><span class="text_4">black </span><span class="text_5">--</span><span class="text_4">&gt; </span><span class="text_6">0</span></p>
	<p class="block_27"><span class="text_4">green </span><span class="text_5">--</span><span class="text_4">&gt; </span><span class="text_6">1</span></p>
	<p class="block_27"><span class="text_4">red </span><span class="text_5">--</span><span class="text_4">&gt; </span><span class="text_6">2</span></p>
	<p class="block_27"><span class="text_4">white </span><span class="text_5">--</span><span class="text_4">&gt; </span><span class="text_6">3</span></p>
	<p class="block_27"><span class="text_4">yellow </span><span class="text_5">--</span><span class="text_4">&gt; </span><span class="text_6">4</span></p>
	<p class="block_28">&nbsp;</p>
	<p class="block_27"><span class="text_4">Labels = [</span><span class="text_7">'green'</span><span class="text_4">, </span><span class="text_7">'red'</span><span class="text_4">, </span><span class="text_7">'black'</span><span class="text_4">]</span></p>
	<p class="block_27"><span class="text_4">Encoded values = [</span><span class="text_6">1</span><span class="text_4">, </span><span class="text_6">2</span><span class="text_4">, </span><span class="text_6">0</span><span class="text_4">]</span></p>
	<p class="block_28">&nbsp;</p>
	<p class="block_27"><span class="text_4">Encoded values = [</span><span class="text_6">3</span><span class="text_4">, </span><span class="text_6">0</span><span class="text_4">, </span><span class="text_6">4</span><span class="text_4">, </span><span class="text_6">1</span><span class="text_4">]</span></p>
	<p class="block_27"><span class="text_4">Decoded labels = [</span><span class="text_7">'white'</span><span class="text_4">, </span><span class="text_7">'black'</span><span class="text_4">, </span><span class="text_7">'yellow'</span><span class="text_4">, </span><span class="text_7">'green'</span><span class="text_4">]</span></p>
	<p class="block_13">&nbsp;</p>
	<p class="block_7">Phân loại hồi quy lý luận( Logistic Regression classifier)</p>
	<p class="block_8">Lý luận hồi quy là kỹ thuật sử dụng để giải thích mối quan hệ giữa biến đầu vào và biến đầu ra. Các biến đầu vào được coi là độc lập và các biến đầu ra được coi là biến phụ thuộc. Biến phụ thuộc chỉ có thể lấy một giá trị cố định của các giá trị đầu ra. Các giá trị này tương ứng với các lớp của phần phân loại.</p>
	<p class="block_8">Nhiệu vụ của chúng ta là định danh ( nhận định) mối quan hệ giữa các biến độc lập và phụ thuộc bằng cách ước tính xác xuất sửa dụng một hàm logistic. Hàm logistic này là một <b class="calibre2">đường cong sigmoid(sigmoid curve) </b> nó dùng để xây dựng một hàm cùng với những tham số khác nhau. Nó rất giống với phân tích Linear model,cách mà chúng ta cố gắng đặt một dòng vào một đống điểm để giảm thiểu lỗi. Thay vì sử dụng hồi quy tuyến tính ( linear regression), chúng ta sử dụng hồi quy lý luận( logistic regression). Hồi quy lý luận về cơ bản không phải là một kỹ thuật phân loại ( not a classification technique), nhưng chúng ta sử dụng nó theo cách này để tạo điều kiện cho phân loại. Nó được sử dụng rất thường xuyên trong máy học bởi nó rất cơ bản và đơn giản. Hãy xem làm thế nào để xây dựng một hàm phân loại xử dụng Hồi quy ly luận. Hãy chắc chắn bạn có <i class="calibre5">Tkinter </i>package được cài trong hệ thống trước khi chúng ta bắt đầu. Nếu bạn không có bạn có thể tìm nó ở : <a href="https://docs.python.org/2/library/tkinter.html" class="text_8">https://docs.python.org/2/library/tkinter.html</a>.</p>
	<p class="block_8">Tạo một file Python mới và import các package . Chúng ta sẽ import một hàm từ file utilities.py. Chúng ta sẽ xem sét hàm ở trong đó sau, và bay giờ thì import nó vào nào:</p>
	<div class="frame_"><p class="block_11">import numpy as np</p><p class="block_17">from sklearn import linear_model</p><p class="block_12">form utilities import visualize_classifier</p></div>
	<p class="block_13">&nbsp;</p>
	<p class="block_8">Define một vài dữ liệu đầu vào với vectors 2 chiều và dán nhãn tương ứng:</p>
	<div class="frame_"><p class="block_30"><i class="calibre3"><span class="calibre4">#  Define</span></i><i class="calibre3"><span class="calibre4"> sample  input data</span></i></p><p class="block_17">X = np.array([[3.1, 7.2], [4, 6.7], [2.9, 8], [5.1, 4.5], [6, 5], [5.6, 5],</p><p class="block_17">[3.3, 0.4], [3.9, 0.9], [2.8, 1], [0.5, 3.4], [1, 4], [0.6, 4.9]])</p><p class="block_12">y = np.array([0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3])</p></div>
	<p class="block_8"><span class="calibre1">Chúng ta sẽ train bộ phân loại bằng cách sử dụng nhãn dữ liệu.  Giờ thì tạo một Lý luận hồi quy object:</span></p>
	<div class="frame_"><p class="block_11"># create the logistic regression classifier</p><p class="block_12">classifier = linear_model.LogisticRegression(solver=’liblinear’,C=1)</p></div>
	<p class="block_8">Huấn luyện bộ phân loại sử dụng dữ liệu chúng ta đã định nghĩa ban đầu:</p>
	<div class="frame_"><p class="block_11"># Train the classifier</p><p class="block_12">classifier.fit(X,y)</p></div>
	<p class="block_8">Nhắc lại sự thực hiện phân loại bằng cách nhìn vào ranh giới của các lớp:</p>
	<div class="frame_"><p class="block_11"># Visualize the performance of the classifier</p><p class="block_12">visualize_classifier(classifier, X, y)</p></div>
	<p class="block_8">Chúng ta cần định nghĩa hàm này trước khi chúng ta dùng nó. Chúng ta sẽ sử dụng nhiều lần trong chapter này vị vậy tốt nhất là chúng ta định nghĩa nó trong một file và import hàm đó vào. Hàm này được đưa vào trong file utilities.py .</p>
	<p class="block_8">Tạo file Python đặt tên là utilities.py.</p>
	<div class="frame_"><p class="block_11">Import numpy as np</p><p class="block_17">Import matplotlib.pyplot as plt</p><p class="block_18"><i class="calibre3"><span class="calibre4">def  visualize</span></i><i class="calibre3">_classifier(classifier,X,y):</i></p><p class="block_31"># ĐỊnh nghĩa gí trị lớn nhất và nhỏ nhất cho X và y</p><p class="block_31"># nó sẽ được sử dụng trong một lưới</p><p class="block_31">min_x,max_x=X[:,0].min() -1.0,X[:,0].max()+1.0</p><p class="block_26">min_y,max_y=X[:,1].min()-1.0,X[:,1].max()+1.0</p></div>
	<p class="block_8">Chúng ta đã xác định giá trị nhỏ nhất và lớn nhất của các hướng X và Y sẽ được sử dụng trong lưới của chúng ta . Lưới này về cơ bản là một tập hợp các giá trị được sử dụng để dánh giá các hàm, để chúng ta có thể hình dung được ranh giới của các lớp . Xác định kích thước bước cho lưới và tạo cho nó các giá trị tối thiểu và tối đa:</p>
	<p class="block_13">&nbsp;</p>
	<div class="frame_"><p class="block_11"># Xác định kích thước bước cho lưới( </p><p class="block_17">mesh_stop_size=0.01</p><p class="block_17"># Define lưới cho các giá trị X và Y</p><p class="block_12">x_vals,y_vals=np.meshgrid(np.arange(min_x,max_x,mesh_step_size),np.arange(min_y,max_y,mesh_step_size))</p></div>
	<p class="block_8">Chạy bộ phân loại trên tất cả các điểm của lưới</p>
	<div class="frame_"><p class="block_11"># Chạy bộ phân loại trên lưới</p><p class="block_17">output=classifier.predict(np.c_[x_vals,ravel(),y_vals.ravel()])</p><p class="block_17"># Xếp loại array đầu ra</p><p class="block_12">output= ouput.reshape(x_vals.shape)</p></div>
	<p class="block_8">Tạo một figure( giao diện đồ họa trong matplot), chọn một màu và đổ lên trên các điểm:</p>
	<div class="frame_"><p class="block_11"># tạo một plot figure</p><p class="block_17">plt.figure()</p><p class="block_17">#Chọn màu cho plot</p><p class="block_17">plt.pcolormesh(x_vals,y_vals,output,cmap=plt.cm.gray)</p><p class="block_17"># Đổ màu các điểm huấn luyện trên plot</p><p class="block_17">plt.scatter(X[:,0],X[:,1],c=y,s=75,edgecolors=’black’,linewidth=1,cmap=plt.cm.Paired)</p><p class="block_17"># Chỉ ra các điểm viền bao quanh của plots sử dụng giá trị nhỏ nhất và lớn nhất, them tick marks và hiển thị figure </p><p class="block_17"># Điểm viền của plot</p><p class="block_17">plt.xlim(x_vals.min(),x_vals.max())</p><p class="block_17">plt.ylim(y_vals.min(),y_vals.max())</p><p class="block_17"># Chỉ địch đánh dấu trên trục X và Y</p><p class="block_17">plt.xticks((np.arange(int(X[:,0].min()-1),int(X[:,0].max()+1),1.0)))</p><p class="block_17">plt.yticks((np.arange(int(X[:,1].min()-1),int(X[:,1].max()+1,1.0)))</p><p class="block_12">plt.show();</p></div>
	<p class="block_13">&nbsp;</p>
	<p class="block_13">&nbsp;</p>
	<p class="block_8">Ok chạy xong rồi thì ta sẽ có hình như sau </p>
	<p class="block_8"><img alt="Image" src="images/000003.png" class="calibre6" /></p>
	<p class="block_8">Ok giờ thay đổi giá trị C thành 100 trong object classifier </p>
	<p class="block_32">classifier = linear_model.LogisticRegression(solver=’liblinear’,C=100)</p>
	<p class="block_8">Lý do giá trị C áp dụng loại bỏ phân loại sai, nên thuật toán sẽ phân loại tốt hơn để training dữ liệu. Bạn nên cẩn thận với tham số này, bởi vì nếu bạn tang quá nhiều nó sẽ không phù hợp với training dât, và nó sẽ không phù hợp.</p>
	<p class="block_8">Sau khi thay đổi tham số C thành 100 chạy thử lại file logistic_regression.py ta sẽ có kết quả như sau:</p>
	<p class="block_33"><img alt="Image" src="images/000000.png" class="calibre7" /></p>
	<p class="block_13">&nbsp;</p>
	<p class="block_8">Nếu so với figure phía trên bạn sẽ thấy đường viền xung quanh có vẻ nhìn tốt hơn.</p>
	<p class="block_7">Naïve Bayes classifier</p>
	<p class="block_13">&nbsp;</p>
	<p class="block_8">Naïve Bayes là một thuật toán để xây dựng phân loại sử dụng định lý Bayes. Định lý Bayes là một kết quả của lý thuyết xác suất. Định lý Bayes cho phép tính xác suất xảy ra của một sự kiện ngẫu nhiên A<span class="calibre1"> khi biến sự kiện liên quan B đã xảy ra. Xác xuất này được ký hiệu là P(A|B) và đọc là: xác  suất của A nếu có B. Đại lượng này được gọi là xác suất điều kiện hay xác suất hậu nghiệm vì nó được rút ra từ giá trị được cho của B hoặc phụ thuộc vào giá trị đó:</span></p>
	<p class="block_8">Theo định lý Bayes, xác suất xảy ra A khi biết B sẽ phụ thuộc vào 3 yếu tố:</p>
	<ul class="list_">
	<li class="block_4">Xác xuất xẩy ra A của riêng nó ,<span class="calibre1"> không quan tâm đến B. Ký hiệu là P(A)  và đọc là xác suất của A. Đây được gọi là xác suất biên duyên hay xác suất tiên nghiệm</span> (prior). gọi là tiên nghiệm theo nghĩa rằng nó không quan tâm đến bất kỳ thông tin nào về B</li>
	<li class="block_5">Xác suất xảy ra B của riêng nó, không quan tâm đến A. Ký hiệu là P(B) đọc là : xác suất của B. Đại lượng này gọi là hằng số chuẩn hóa (normalizing constant), vì nó luôn giống nhau, không phụ thuộc vào sự kiện A đang muốn biết.</li>
	<li class="block_6">Xác suất xảy ra B khi biết A xảy ra . Ký hiệu là P(B|A) và đọc là “xác suất của B nếu có A”. Đại lượng này gọi là <i class="calibre5">khả năng (</i>likehood) xảy ra B khi biết A đã xyar ra. Chú ý không nhầm lẫn giữa khả năng xảy ra B khi biết A và xác suất xảy ra A khi biết B.</li>
</ul>
	<p class="block_34">Khi biết ba đại lượng ày, xác suất của A khi biết B cho bởi công thức:</p>
	<p class="block_35">&nbsp;</p>
	<p class="block_34">Đọc them tại wiki <a href="https://vi.wikipedia.org/wiki/%C4%90%E1%BB%8Bnh_l%C3%BD_Bayes" class="text_8">https://vi.wikipedia.org/wiki/%C4%90%E1%BB%8Bnh_l%C3%BD_Bayes</a></p>
	<p class="block_34">Chúng ta xây dựng một bộ phân loại NaiveBayes bằng cách gắn nhãn dán cho class cho từng trường hợp vấn đề. Những trường hợp vấn <span class="calibre1">đề  được</span> biểu diễn như những vector giá trị nhiều chiều. Sau khi tính toán xác suất chuẩn hóa( P(B) ) cho một số giả thuyết khác nhau bạn có thể chọn giả thuyết có xác suất cao nhất. Đây là giả thuyết có thể thể xảy ra tối đa và có thể là chính thức được gọi là giả thuyết tối đa (MAP). Ta có công thức : MAP(B)=max(P(A|B)*P(B)).</p>
	<p class="block_34">Giả định ở đây là giá trị của bất kỳ đặc tính được đưa vào là độc lập với giá trị của các đặc tình khác. Đây được gọi là giả định độc lập.</p>
	<p class="block_34">Xác suất lớp, chúng ta có thể thấy một tính năng giả định ảnh hưởng thế nào bất kể ảnh hưởng của nó đến các tính năng khác. Ví dụ : xét một con vật. nó có thể được coi là một con báo nếu nó lốm đốm, có 4 chân và có 1 cái đuôi, có thể chạy khoảng 70 dặm/giờ. Phân loại Naïve Bayes xem xét các tính năng độc lập để cho ra kết quả. Kết quả chỉ ra là con vật đó chính là một con báo. Chúng ta không quan tâm xem xét đến mối quan hệ có thể xảy ra giữa sự lốm đốm của da, số chân, số đuôi và tốc độ di chuyển. Bây giờ thì xây dựng một bộ phân loại Naïve Bayes bằng python.</p>
	<p class="block_34">Tạo 1 file python và import đống package sau:</p>
	<div class="frame_"><p class="block_30"><i class="calibre3"><span class="calibre4">import  numpy</span></i><i class="calibre3"> as np</i></p><p class="block_17">import matplotlib.pyplot as plt</p><p class="block_17">from sklearn.Naive_bayes import GaussianNB</p><p class="block_17">form sklearn import cross_validation</p><p class="block_36"></p><p class="block_12">form utilities import visiualize_classifier</p></div>
	<p class="block_8">Chúng ta sử dụng một file txt giống như là một dữ liệu nguồn, file này chứa các giá trị được cách nhau bởi dấu phẩy trong mỗi dòng. File txt có dạng như sau :</p>
	<p class="block_37">2.18,0.57,0<br class="calibre8" />4.13,5.12,1<br class="calibre8" />9.87,1.95,2<br class="calibre8" />4.02,-0.8,3<br class="calibre8" />1.18,1.03,0<br class="calibre8" />4.59,5.74,1<br class="calibre8" />8.25,1.3,2<br class="calibre8" />3.91,-0.68,3</p>
	<p class="block_8">Rầu giờ khai báo một biến và đọc file txt này với numpy</p>
	<div class="frame_"><p class="block_11">input_file = ‘data_multiva_nb.txt’</p><p class="block_17">data=np.loadtxt(input_file,delimiter=’,’)</p><p class="block_12">X,y=data[:,:-1],data[:,1]</p></div>
	<p class="block_8">Tạo một instance của Class phân loại Naïve bayes. Chúng ta xử dụng Gaussian Naïve Bayes ở đây. Trong dạng phân loại này chúng ta giả định giá trị liên quan trong mỗi lớp theo Gaussian và train phân loại (train the classifier) với dữ liệu đã cho:</p>
	<div class="frame_"><p class="block_11">classifier= GaussianNB()</p><p class="block_12">classifier.fit(X,y)</p></div>
	<p class="block_38">(<span class="text_9">Naive Bayes có thể được mở rộng thành các thuộc tính có giá trị thực, phổ biến nhất bằng cách giả sử phân phối Gaussian.</span></p>
	<p class="block_38"><span class="text_9">Phần mở rộng này của Naive Bayes được gọi là Gaussian Naive Bayes. Các hàm khác có thể được sử dụng để ước tính phân phối dữ liệu, nhưng Gaussian (hoặc phân phối chuẩn) là dễ nhất để làm việc vì bạn chỉ cần ước tính giá trị trung bình và độ lệch chuẩn từ dữ liệu đã train của bạn.</span>)</p>
	<p class="block_8">Chạy classifier phân loại dữ liệu đã được train và dự đoán đầu ra:</p>
	<p class="block_32">y_pred=classifier.predict(X)</p>
	<p class="block_8">Hãy tính độ chính xác của cách phân loại bằng cách so sánh giá trị dự đoán cùng với một nhãn dán thực, và sau đó thể hiện nó ra bằng đồ thị</p>
	<div class="frame_"><p class="block_11">accuracy=100.0*(y=y_pred).sum()/X.shape[0]</p><p class="block_17">print(“Accuracy of Naïve Bayes classifier=”,round(accuracy,2),”%”)</p><p class="block_17">#visualize</p><p class="block_12">visualize_classifier(classifier,X,y)</p></div>
	<p class="block_39"><span class="text_10"><img alt="Image" src="images/000006.png" class="calibre9" /> </span><span class="text_11">Console: Accuracy of Naive Bayes classifier = 99.75 $)</span></p>
	<p class="block_8">Cách dự đoán bằng phương thức predict cũng không phải là mạnh. Chúng ta cần phải thực hiện xác nhận chéo để chúng không sử dụng cùng một dữ liệu đã được train khi chúng ta test.</p>
	<p class="block_8">Chia dữ liệu đã được train và dữ liệu con để test. Giống như chỉ định bởi tham số <i class="calibre5">test_size </i>ở dòng code bên dưới, Chúng ta sẽ phân bổ chúng thành 80% để dành cho train và 20% dành cho test. Sau đó chúng ta train lại dữ liệu này với Naïve Bayes:</p>
	<div class="frame_"><p class="block_11"># Split data into training and test data</p><p class="block_17">X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y,</p><p class="block_17">test_size=0.2, random_state=3)</p><p class="block_17">classifier_new = GaussianNB()</p><p class="block_17">classifier_new.fit(X_train, y_train)</p><p class="block_12">y_test_pred = classifier_new.predict(X_test)</p></div>
	<p class="block_8">Giờ tính lại độ chính xác của phân loại và biểu thị trên biểu đồ </p>
	<p class="block_13">&nbsp;</p>
	<div class="frame_"><p class="block_11"># compute accuracy of the classifier</p><p class="block_17">accuracy = 100.0 * (y_test == y_test_pred).sum() / X_test.shape[0]</p><p class="block_17">print("Accuracy of the new classifier =", round(accuracy, 2), "%")</p><p class="block_17"># Visualize the performance of the classifier</p><p class="block_12">visualize_classifier(classifier_new, X_test, y_test)</p></div>
	<p class="block_13">&nbsp;</p>
	<p class="block_33"><img alt="Image" src="images/000007.png" class="calibre10" /></p>
	<p class="block_40">Printed out : (Accuracy of Naive Bayes classifier = 99.75 $</p>
	<p class="block_40">Accuracy of the new classifier = 100.0 %)</p>
	<p class="block_8">và giờ thì sử dụng những hàm có sẵn để tính độ chính xác , xác suất và gọi lại giá trị gấp 3 lần </p>
	<div class="frame_"><p class="block_11">num_folds = 3</p><p class="block_17">accuracy_values = cross_validation.cross_val_score(classifier,</p><p class="block_17">X, y, scoring='accuracy', cv=num_folds)</p><p class="block_17">print("Accuracy: " + str(round(100*accuracy_values.mean(), 2)) + "%")</p><p class="block_17">precision_values = cross_validation.cross_val_score(classifier,</p><p class="block_17">X, y, scoring='precision_weighted', cv=num_folds)</p><p class="block_17">print("Precision: " + str(round(100*precision_values.mean(), 2)) + "%")</p><p class="block_17">recall_values = cross_validation.cross_val_score(classifier,</p><p class="block_17">X, y, scoring='recall_weighted', cv=num_folds)</p><p class="block_17">print("Recall: " + str(round(100*recall_values.mean(), 2)) + "%")</p><p class="block_17">f1_values = cross_validation.cross_val_score(classifier,</p><p class="block_17">X, y, scoring='f1_weighted', cv=num_folds)</p><p class="block_12">print("F1: " + str(round(100*f1_values.mean(), 2)) + "%")</p></div>
	<p class="block_8">chạy tiếp đoạn code trên ta sẽ có kết quả như sau </p>
	<div class="frame_1"><p class="block_20">Accuracy: 99.75%</p><p class="block_21">Precision: 99.76%</p><p class="block_21">Recall: 99.75%</p><p class="block_22">F1: 99.75%</p></div>
	<p class="block_14"><span class="text_12">Confusion matrix </span><span class="text_10">(Ma trận hỗn loạn)</span></p>
	<p class="block_8">Một ma trận hỗn loạn là một biểu đồ hoặc một bảng dùng đề biểu diễu một lớp phân loại. Nó thường được sử dụng để phân tích từ một tập dữ liệu thử nghiệm (test dataset) nơi mà <span class="text_3">ground truth </span>đã được biết trước. Ở đây ta có một khái niệm “Ground Truth” về cơ bản ground truth là nhãn/đầu ra<i class="calibre5"> thực sự </i>của các điểm trong test dataset, khái niệm này được dùng nhiều trong ML. Chúng ta so sánh mỗi class cùng với nhau và sẽ nhìn thấy bao nhiêu mẫu thử đã phân loại sai. Trong suốt quá trình xây dựng bảng này, chúng ta sẽ đi qua một vài key metrics (từ khóa chính) những thứ vô cùng quan trọng trong lĩnh vực ML. Chúng ta sẽ xem xét mã nhị phân phân loại nơi mà đầu ra chỉ là 0 hoặc 1:</p>
	<ul class="list_">
	<li class="block_4"><b class="calibre2">True positives TP:</b> Đây là mẫu thử mà chúng ta dự đoán kết quả sẽ là 1 ở đầu ra và <i class="calibre5">kết quả thực sự</i> là 1</li>
	<li class="block_5"><b class="calibre2">True negatives TN:</b> Những mẫu thử mà chúng ta dự đoán là 0 (predict 0) ở đầu ra và <i class="calibre5">kết quả thực sự</i> là 0</li>
	<li class="block_5"><b class="calibre2">False positives FP:</b> <b class="calibre2"> </b>Đây là mẫu thử mà chúng ta dự đoán giá trị là 1 ở đầu ra nhưng <i class="calibre5">kết quả thực sự </i>là 0. Cái này còn được biết đến là Type I error.</li>
	<li class="block_6"><b class="calibre2">False negatives FN: </b>Đây là mẫu thử mà chúng ta dự đoán giá trị là 0 ở đầu ra nhưng kết quả thực sự là 1. Cái này còn được gọi là Type II error.</li>
</ul>
	<p class="block_13">&nbsp;</p>
	<p class="block_8">Trong bài toán phân loại sau khi đã xử lý dữ liệu và đưa nó vào mô hình ( preprogcess<span class="calibre1"> and  model) thì đầu ra của mô hình này sẽ là một tập vector xác xuất tương ứng của từng lớp. Ta có thể đánh giá mức độ chính xác của mô thinh thông qua chỉ số về độ chính xác (accuracy). </span>vd ta có một bài toán chuẩn đoán hành lý mang bom trong 100 túi hành lý. ví dụ trong 100 túi đó có 1 túi mang bom mà mô hình của chúng ta dự đoán 100 hành lý đều không có bom thì độ chính xác của mô hình này là 99% siêu cao phải ko? nhưng với 1% là đủ tèo rồi nên chúng ta cần có một cách giải khác đó là confusion matrix. Confusion Matrix là một phương pháp đánh giá kết quả của những bài toán phân loại, với việc xem xét cả những chỉ số về độ chính xác và độ bao quát của các dự đoán cho từng lớp.</p>
	<p class="block_8">Tùy thuộc vào những vấn để gặp phải, chúng ta có thể tối ưu thuật toán của mình để giám tỷ lệ gặp phải FP và FN. Ví dụ, trong một hệ thống nhận dạng sinh trắc học (biometric identification), Chúng ta cần tránh gặp phải FP, điều này rất quan trọng vì có thể gặp phải những người xấu khai thác thông tin cá nhân. Từ 4 chỉ số trên ta có 2 con số để đánh giá mức độ tin cậy của một mô hình :</p>
	<ul class="list_">
	<li class="block_4"><b class="calibre2">Precision:</b> Trong tất cả các dự đoán Positive được đưa ra bao nhiêu dự đoán là chính xác? Chỉ số này được tính theo công thức :  </li>
	<li class="block_6"><b class="calibre2">Recall: </b>Trong tất cả các trường hợp Positive, bao nhiêu trường hợp đã được dự đoán chính xác? Chỉ số này được tính theo công thức : </li>
</ul>
	<p class="block_8">Quay trở lại bài toán tìm hành lý . Giả sử ta có 1 tập vector dữ liệu gồm 100 dữ liệu hành lý với 99 hành lý ok ( Negative) và 10 hành lý có vấn đề ( Positive) và mô hình của chúng ta dự đoán 2/10 hành lý có vấn đề tức là đưa ra dự đoán 2 hành lý có vấn đề thì cả 2 đều chính xác. Như vậy chỉ số Precision khi dự đoán lớp hành lý có vấn đề là 1. Tuy nhiên 8/10 hành lý còn lại bị bỏ qua nên chỉ số recall chỉ là 0.2. Để đánh giá độ tin cậy chung của mô hình người ta kết hợp 2 chỉ số <b class="calibre2">Precision</b> và <b class="calibre2">Recall </b>thành một chỉ số duy nhất là <b class="calibre2">F-Score</b> : ta có công thức : </p>
	<p class="block_8">Một mô hình có chỉ số F-score cao chỉ khi cả 2 chỉ số Precision và Recall để cao. Một trong 2 chỉ số này thấp đều sẽ kéo điểm F-score xuống. Trường hợp xấu nhất khi 1 trong hai chỉ số Precison và Recall bằng 0 sẽ kéo điểm F-score về 0. Trường hợp tốt nhất khi cả điểm chỉ số đều đạt giá trị bằng 1, khi đó điểm F-score sẽ là 1.</p>
	<p class="block_8">Qua việc sử dụng chỉ số F-score, ta đã có một thước đo đáng tin cậy về hiệu năng của mô hình trong các bài toán phân loại, đặc biệt khi dữ liệu về một lớp lớn hơn gấp nhiều lần so với dữ liệu về lớp còn lại </p>
	<p class="block_41">(Nguồn confusion matrix lấy từ towardsdatascience Understanding Confusion Matrix dịch bởi bạn Nguyễn Hoàng Nam )</p>
	<p class="block_13">&nbsp;</p>
	<p class="block_8">Okey giờ thì tạo thử một ma trận hỗn loạn (confusion matrix).Rầu bắt đầu tạo một file Python mới và import các package cần thiết:</p>
	<div class="frame_"><p class="block_11">import numpy as np</p><p class="block_17">import matplotlib.pylot as pit</p><p class="block_17">from sklearn.metrics import confusion_matrix</p><p class="block_17">from sklearn.metrics import classification_report</p><p class="block_18"><i class="calibre3"># Định nghĩa một vài nhãn mẫu <span class="calibre4">thử  cho</span></i><i class="calibre3"> kết quả thực sự và dự đoán kết quả đầu ra:</i></p><p class="block_17">true_label=[2,0,0,2,4,4,1,0,3,3,3]</p><p class="block_17">pred_label=[2,1,0,2,4,3,1,0,1,3,3]</p><p class="block_17"># Tạo một confusion matrix</p><p class="block_17">confusion_mat=confusion_matrix(true_label,pred_label]</p><p class="block_17"># In confusion matrix ra màn hình</p><p class="block_17">plt.imshow(confusion_mat,interpolation=’nearest’,cmap=plt.cm.gray)</p><p class="block_17">plt.title(‘confusion matrix’)</p><p class="block_17">plt.colorbar()</p><p class="block_17">ticks=np.arange(5)</p><p class="block_17">plt.xticks(ticks,ticks)</p><p class="block_17">plt.yticks(ticks,ticks)</p><p class="block_17">plt.ylabel(‘True labels’)</p><p class="block_17">pt.xlabel(‘Predict labels’)</p><p class="block_12">plt.show()</p></div>
	<p class="block_33"><img alt="Image" src="images/000002.png" class="calibre11" /></p>
	<p class="block_42">Chạy code trên với PyCharm ta có hình bên trên</p>
	<p class="block_8">Trên hình bên trên, biến <span class="calibre1">ticks  đề</span> cập đến sự phân hóa của các lớp. trong trường hợp này chúng ta dùng 5 nhãn riêng biệt. Màu trắng thể hiện giá trị cao hơn, trong khi màu đen thể hiện giá trị thấp hơn . Trong trường hợp lý tưởng thì tất cả các ô sẽ là màu trắng và các ô còn lại sẽ là màu đen, nó thể hiện độ chính xác là 100%.</p>
	<p class="block_8">Giờ thì in báo cáo phân loại ra:</p>
	<div class="frame_"><p class="block_11"># Classification report</p><p class="block_17">target =[‘Class-0’,’Class-1’,’Class-2’,’Class-3’,’Class-4’]</p><p class="block_12">print(‘\n’,classification_report(true_label,pred_label,target_names=target))</p></div>
	<p class="block_13">&nbsp;</p>
	<p class="block_8">Kết quả in ra ở Terminal:</p>
	<div class="frame_1"><p class="block_20"><span class="calibre1">               </span>precision<span class="calibre1">    recall  f1-score   support</span></p><p class="block_43"></p><p class="block_21"><span class="calibre1">     Class-0       1.00      0.67      0.80         3</span></p><p class="block_21"><span class="calibre1">     Class-1       0.33      1.00      0.50         1</span></p><p class="block_21"><span class="calibre1">     Class-2       1.00      1.00      1.00         2</span></p><p class="block_21"><span class="calibre1">     Class-3       0.67      0.67      0.67         3</span></p><p class="block_21"><span class="calibre1">     Class-4       1.00      0.50      0.67         2</span></p><p class="block_21"><span class="calibre1">  </span>micro<span class="calibre1"> avg       0.73      0.73      0.73        11</span></p><p class="block_21"><span class="calibre1">   </span>macro<span class="calibre1"> avg       0.80      0.77      0.73        11</span></p><p class="block_22">weighted<span class="calibre1"> avg       0.85      0.73      0.75        11</span></p></div>
	<p class="block_7">Support Vector Machines</p>
	<p class="block_8">Một Support Vector Machine (SVM) là một kiểu phân loại sử dụng siêu mặt phẳng giữa các lớp. Một siêu mặt phẳng (hyperplane) mà có N-Chiều trên một đường. Đưa ra những dữ liệu đã được training và một vấn đề phân loại nhị phân . SVM là để tìm phương pháp tối ưu cho siêu mặt phẳng mà trên đó sẽ chia những dữ liệu được train thành 2 lớp. Nó dễ dàng mở rộng ra với vấn đề gặp lại với N-Lớp.</p>
	<p class="block_8">Hãy xem xét một vấn đề 2 chiều cùng với 2 lớp điểm. Cái đưa ra là 2D, chúng ta sẽ xem xét giữa các điểm và đường thẳng trong một mặt phẳng 2D, Cái này để dễ dàng diễn tả hơn là vector và siêu mặt phẳng trong một không gian nhiều chiều. Tất nhiên đây là một phiên bản cơ bản của cách phân loại SVM, nhưng nó rất quan trọng để hiểu nó và tưởng tượng ra nó trước khi chúng ta có thể áp dụng nó vào dữ liệu nhiều chiều.</p>
	<p class="block_8">Xem xét hình đồ thị dưới đây: </p>
	<p class="block_8"><img alt="Image" src="images/000004.png" class="calibre12" /></p>
	<p class="block_8">ở đây có 2 lớp ( class0,class1) gồm những điểm mà chúng ta muốn tìm siêu mặt phẳng tối ưu để chia chúng thành 2 lớp. Nhưng làm sao chúng ta có thể định nghĩa là tối ưu ?<span class="calibre1"> Trên hình bên trên đường thẳng kẻ đậm là đường thẳng giới thiệu về siêu mặt phẳng. Bạn có thể vẽ nhiều đường thẳng để chia 2 lớp tập hợp 2 điểm nhưng đường thẳng kẻ đậm là đường thẳng phân chia tốt nhất, bởi nó chứa những điểm có khoảng cách đều nhất tới các điểm được phân chia. Những điểm chấm trên dòng kẻ chấm chấm được gọi là Support Vector.  Những khoảng cách vuông góc giữa hai đường kẻ chấm được gọi là lề tối </span>đa(maximum margin).</p>
	<p class="block_13">&nbsp;</p>
	<p class="block_7">Phân loại dữ liệu Thu Nhập sử dụng SVM</p>
	<p class="block_8">Chúng ta sẽ xây dựng một class SVM để dự đoán mức thu nhập của một người dựa trên 14 thuộc tính. Thành công mong đợi ở dự án này là tìm ra mức thu nhập của người đó cao hơn hay thấp hơn 50.000$ một năm, do đó đây là một cách phân loại nhị phân ( cao hay thấp). Chúng ta sẽ xử dụng dữ liệu điều tra dân số có sẵn tại trang web <a href="https://archive.ics.uci.edu/ml/datasets/Census+Income" class="text_8">https://archive.ics.uci.edu/ml/datasets/Census+Income</a>. Một thứ cần chú ý dữ liệu ở <span class="calibre1">đây  là</span> dữ liệu này là một <i class="calibre5">datapoint </i>nó là một hỗn hợp xáo trộn gồm chữ và số . Chúng ta không thể xử dụng dữ liệu này lập tức dưới dạng của nó được, bởi vì thuật toán không biết làm cách nào để xử lý dữ liệu chữ. Chúng ta cũng không thể convert mọi thứ sửa dụng mã hóa nhãn (label encoder) bởi vì những số ở đó có giá trị. Vì thế chúng ta cần xử dụng kết hợp giữa mã hóa nhãn và dữ liệu số thô để xây dựng một lớp phân loại hiệu quả nhất:</p>
	<p class="block_8">Tạo 1 file Python mới và import các package:</p>
	<div class="frame_"><p class="block_44"><span class="text_13">import </span><span class="text_14">numpy </span><span class="text_13">as </span><span class="text_14">np<br class="calibre8" /><br class="calibre8" /></span><span class="text_13">from </span><span class="text_14">sklearn </span><span class="text_13">import </span><span class="text_14">preprocessing<br class="calibre8" /></span><span class="text_13">from </span><span class="text_14">sklearn.svm </span><span class="text_13">import </span><span class="text_14">LinearSVC<br class="calibre8" /></span><span class="text_13">from </span><span class="text_14">sklearn.multiclass </span><span class="text_13">import </span><span class="text_14">OneVsOneClassifier<br class="calibre8" /></span><span class="text_13">from </span><span class="text_14">sklearn </span><span class="text_13">import </span><span class="text_14">model_selection<br class="calibre8" /></span><span class="text_15"># Chúng ta xử dụng file income_data.txt làm dữ liệu. file này chứa dữ liệu thu nhập chi tiết:<br class="calibre8" /></span><span class="text_14">input_file =</span><span class="text_16">"data/income_data.txt"<br class="calibre8" /></span><span class="text_15"># Theo thứ tự tải dữ liệu từ file, chúng ta cần xử lý nó trước vì vậy nên chúng ta sẽ cần chuẩn bị để phân loại. Chúng ta sẽ dùng 25000 điểm dữ liệu cho mỗi lớp:<br class="calibre8" /></span><span class="text_14">X=[]<br class="calibre8" />y=[]<br class="calibre8" />count_class1=</span><span class="text_17">0<br class="calibre8" /></span><span class="text_14">count_class2=</span><span class="text_17">0<br class="calibre8" /></span><span class="text_14">max_datapoints=</span><span class="text_17">25000<br class="calibre8" /></span><span class="text_15"># Mở file và bắt đầu đọc các dòng<br class="calibre8" /></span><span class="text_13">with </span><span class="text_18">open</span><span class="text_14">(input_file</span><span class="text_13">,</span><span class="text_16">'r'</span><span class="text_14">) </span><span class="text_13">as </span><span class="text_14">f:<br class="calibre8" /><span class="calibre4">   </span></span><span class="text_13">for </span><span class="text_14">line </span><span class="text_13">in </span><span class="text_14">f.readlines():<br class="calibre8" /><span class="calibre4">      </span></span><span class="text_13">if </span><span class="text_14">count_class1&gt;=max_datapoints </span><span class="text_13">and </span><span class="text_14">count_class2&gt;=max_datapoints:<br class="calibre8" /><span class="calibre4">         </span></span><span class="text_13">break</span><span class="text_14">;<br class="calibre8" /><span class="calibre4">      </span></span><span class="text_13">if </span><span class="text_16">'?' </span><span class="text_13">in </span><span class="text_14">line:<br class="calibre8" /><span class="calibre4">         </span></span><span class="text_13">continue<br class="calibre8" /></span><span class="text_15"># mỗi dòng được chia ra bằng dấu phẩy vì thế chúng ta cần chia nó một cách phù hợp. thành phần cuối cùng trong mỗi dòng đại diện cho nhãn. Dựa trên nhãn đó chúng ta sẽ chỉ định nó vào một lớp:<br class="calibre8" /><br class="calibre8" /><span class="calibre4">      </span></span><span class="text_14">data=line[:-</span><span class="text_17">1</span><span class="text_14">].split(</span><span class="text_16">', '</span><span class="text_14">)<br class="calibre8" /><span class="calibre4">      </span></span><span class="text_13">if </span><span class="text_14">data[-</span><span class="text_17">1</span><span class="text_14">]==</span><span class="text_16">'&lt;=50K' </span><span class="text_13">and </span><span class="text_14">count_class1&lt;max_datapoints:<br class="calibre8" /><span class="calibre4">         X.append(data)</span></span><span class="text_14"><br class="calibre8" /><span class="calibre4">         count_class1 +=</span></span><span class="text_17">1<br class="calibre8" /><span class="calibre4">      </span></span><span class="text_13">if </span><span class="text_14">data[-</span><span class="text_17">1</span><span class="text_14">]==</span><span class="text_16">'&gt;50K' </span><span class="text_13">and </span><span class="text_14">count_class2&lt;max_datapoints:<br class="calibre8" /><span class="calibre4">         X.append(data)</span></span><span class="text_14"><br class="calibre8" /><span class="calibre4">         count_class2 +=</span></span><span class="text_17">1<br class="calibre8" /><br class="calibre8" /></span><span class="text_15"># convert X list này dưới dạng numpy array chúng ta đã có dữ liệu đầu vào cho hàm sklearn<br class="calibre8" /></span><span class="text_14">X=np.array(X)<br class="calibre8" /><br class="calibre8" /></span><span class="text_15"># nếu có bất kỳ thuộc tính nào là string ( dữ liệu chữ), khi đó chúng ta cần mã hóa nó. Nếu dữ liệu là số chúng ta có thể giữ nó lại. Chú ý chúng ta kết thúc cùng với nhiều nhãn được mã hóa và chúng ta cần theo dõi tất cả chúng:<br class="calibre8" /></span><span class="text_14">label_encoder = []<br class="calibre8" />X_encoded = np.empty(X.shape)<br class="calibre8" /></span><span class="text_13">for </span><span class="text_14">i</span><span class="text_13">,</span><span class="text_14">item </span><span class="text_13">in </span><span class="text_18">enumerate</span><span class="text_14">(X[</span><span class="text_17">0</span><span class="text_14">]):<br class="calibre8" /><span class="calibre4">   </span></span><span class="text_13">if </span><span class="text_14">item.isdigit():<br class="calibre8" /><span class="calibre4">      X_encoded[:</span></span><span class="text_13">, </span><span class="text_14">i] = X[:</span><span class="text_13">, </span><span class="text_14">i]<br class="calibre8" /><span class="calibre4">   </span></span><span class="text_13">else</span><span class="text_14">:<br class="calibre8" /><span class="calibre4">      label_encoder.append(preprocessing.LabelEncoder())</span></span><span class="text_14"><br class="calibre8" /><span class="calibre4">      X_encoded[:</span></span><span class="text_13">, </span><span class="text_14">i] = label_encoder[-</span><span class="text_17">1</span><span class="text_14">].fit_transform(X[:</span><span class="text_13">, </span><span class="text_14">i])<br class="calibre8" /><span class="calibre4">      </span></span><span class="text_18">print</span><span class="text_14">(X[:</span><span class="text_13">, </span><span class="text_14">i])<br class="calibre8" />X = X_encoded[:</span><span class="text_13">, </span><span class="text_14">:-</span><span class="text_17">1</span><span class="text_14">].astype(</span><span class="text_18">int</span><span class="text_14">)<br class="calibre8" />y = X_encoded[:</span><span class="text_13">, </span><span class="text_14">-</span><span class="text_17">1</span><span class="text_14">].astype(</span><span class="text_18">int</span><span class="text_14">)<br class="calibre8" /><br class="calibre8" />Z=[[</span><span class="text_17">2</span><span class="text_13">,</span><span class="text_17">3</span><span class="text_13">,</span><span class="text_17">1</span><span class="text_14">]</span><span class="text_13">,</span><span class="text_14">[</span><span class="text_17">4</span><span class="text_13">,</span><span class="text_17">5</span><span class="text_13">,</span><span class="text_17">1</span><span class="text_14">]</span><span class="text_13">,</span><span class="text_14">[</span><span class="text_17">6</span><span class="text_13">,</span><span class="text_17">7</span><span class="text_13">,</span><span class="text_17">1</span><span class="text_14">]]<br class="calibre8" />Z=np.array(Z)<br class="calibre8" />D=Z[:</span><span class="text_13">,</span><span class="text_14">-</span><span class="text_17">1</span><span class="text_14">]<br class="calibre8" /></span><span class="text_15">#print(X[:,len(X[0])])<br class="calibre8" /></span><span class="text_13">for </span><span class="text_14">i</span><span class="text_13">,</span><span class="text_14">mitem </span><span class="text_13">in </span><span class="text_18">enumerate</span><span class="text_14">(label_encoder[-</span><span class="text_17">2</span><span class="text_14">].classes_):<br class="calibre8" /><span class="calibre4">   </span></span><span class="text_18">print</span><span class="text_14">(mitem</span><span class="text_13">,</span><span class="text_16">"--&gt;"</span><span class="text_13">,</span><span class="text_14">i)<br class="calibre8" /><br class="calibre8" /><br class="calibre8" /></span><span class="text_18">print</span><span class="text_14">(label_encoder[-</span><span class="text_17">1</span><span class="text_14">].transform([</span><span class="text_16">'&lt;=50K'</span><span class="text_14">]))<br class="calibre8" /></span><span class="text_15"><span class="calibre4"># Tạo lớp phân loại SVM  với một linear kernel</span></span><span class="text_15"><br class="calibre8" /></span><span class="text_14">classifier=OneVsOneClassifier(LinearSVC(</span><span class="text_19">random_state</span><span class="text_14">=</span><span class="text_17">0</span><span class="text_14">))<br class="calibre8" /></span><span class="text_15"># train classifier<br class="calibre8" /></span><span class="text_14">classifier.fit(X</span><span class="text_13">,</span><span class="text_14">y)<br class="calibre8" /></span><span class="text_15"># Thực hiện xác nhận chéo sử dụng cách chia 80/20 để training và testing, sau đó dự đoán kết quả đầu ra cho dữ liệu được training:<br class="calibre8" /># xác nhận chéo<br class="calibre8" /></span><span class="text_14">X_train</span><span class="text_13">, </span><span class="text_14">X_test</span><span class="text_13">, </span><span class="text_14">y_train</span><span class="text_13">, </span><span class="text_14">y_test = model_selection.train_test_split(X</span><span class="text_13">, </span><span class="text_14">y</span><span class="text_13">,</span><span class="text_19">test_size</span><span class="text_14">=</span><span class="text_17">0.2</span><span class="text_13">, </span><span class="text_19">random_state</span><span class="text_14">=</span><span class="text_17">5</span><span class="text_14">)<br class="calibre8" />classifier = OneVsOneClassifier(LinearSVC(</span><span class="text_19">random_state</span><span class="text_14">=</span><span class="text_17">0</span><span class="text_14">))<br class="calibre8" />classifier.fit(X_train</span><span class="text_13">, </span><span class="text_14">y_train)<br class="calibre8" />y_test_pred = classifier.predict(X_test)<br class="calibre8" /></span><span class="text_15"><span class="calibre4"># Tính toán kết quả  F1 cho phân loại :</span></span><span class="text_15"><br class="calibre8" /></span><span class="text_14">f1=model_selection.cross_val_score(classifier</span><span class="text_13">,</span><span class="text_14">X</span><span class="text_13">,</span><span class="text_14">y</span><span class="text_13">,</span><span class="text_19">scoring</span><span class="text_14">=</span><span class="text_16">'f1_weighted'</span><span class="text_13">,</span><span class="text_19">cv</span><span class="text_14">=</span><span class="text_17">2</span><span class="text_14">)<br class="calibre8" /></span><span class="text_18">print</span><span class="text_14">(</span><span class="text_16">"Đẻ lần F1: " </span><span class="text_14">+ </span><span class="text_18">str</span><span class="text_14">(</span><span class="text_18">round</span><span class="text_14">(</span><span class="text_17">100</span><span class="text_14">*f1.mean()</span><span class="text_13">,</span><span class="text_17">2</span><span class="text_14">))+</span><span class="text_16">"%"</span><span class="text_14">)<br class="calibre8" /></span><span class="text_15"># giờ thì classifier đã sẵn sang, hãy xem làm thế nào để lấy ngẫu nhiên một dữ liệu điểm đầu vào và dự đoán kết quả đầu ra. Định nghĩa một điểm data đầu vào như sau:<br class="calibre8" /># đoán dữ liệu đầu ra cho một datapoint test:<br class="calibre8" /></span><span class="text_14">input_data = [</span><span class="text_16">'36'</span><span class="text_13">, </span><span class="text_16">'Private'</span><span class="text_13">, </span><span class="text_16">'215646'</span><span class="text_13">, </span><span class="text_16">'Some-college'</span><span class="text_13">, </span><span class="text_16">'9'</span><span class="text_13">, </span><span class="text_16">'Married-civ-spouse'</span><span class="text_13">,</span><span class="text_16">'Prof-specialty'</span><span class="text_13">, </span><span class="text_16">'Husband'</span><span class="text_13">, </span><span class="text_16">'White'</span><span class="text_13">, </span><span class="text_16">'Male'</span><span class="text_13">, </span><span class="text_16">'0'</span><span class="text_13">, </span><span class="text_16">'0'</span><span class="text_13">, </span><span class="text_16">'40'</span><span class="text_13">,</span><span class="text_16">'Vietnam'</span><span class="text_14">]<br class="calibre8" /></span><span class="text_15"># Trước khi ta thực hiện dự đoán, chúng ta cần mã hóa dữ liệu này bằng cách sử dụng mã hóa nhãn mà chúng ta đã tạo từ trước:<br class="calibre8" /></span><span class="text_14">input_data_encoded = [-</span><span class="text_17">1</span><span class="text_14">] * </span><span class="text_18">len</span><span class="text_14">(input_data)<br class="calibre8" />count = </span><span class="text_17">0<br class="calibre8" /></span><span class="text_13">for </span><span class="text_14">i</span><span class="text_13">, </span><span class="text_14">item </span><span class="text_13">in </span><span class="text_18">enumerate</span><span class="text_14">(input_data):<br class="calibre8" /><span class="calibre4">   </span></span><span class="text_13">if </span><span class="text_14">item.isdigit():<br class="calibre8" /><span class="calibre4">      input_data_encoded[i] = </span></span><span class="text_18">int</span><span class="text_14">(input_data[i])<br class="calibre8" /><span class="calibre4">   </span></span><span class="text_13">else</span><span class="text_14">:<br class="calibre8" /><span class="calibre4">      d=[input_data[i]]</span></span><span class="text_14"><br class="calibre8" /><span class="calibre4">      input_data_encoded[i] = </span></span><span class="text_18">int</span><span class="text_14">(label_encoder[count].transform(d))<br class="calibre8" /><span class="calibre4">      count += </span></span><span class="text_17">1<br class="calibre8" /><br class="calibre8" /></span><span class="text_14">input_data_encoded=np.array(input_data_encoded)<br class="calibre8" /><br class="calibre8" /></span><span class="text_15"># Giờ thì chúng ta đã sẵn sang để dự đoán kết quả đầu ra sử dụng classifier:<br class="calibre8" /></span><span class="text_14">predict_classifier=classifier.predict([input_data_encoded])<br class="calibre8" /></span><span class="text_18">print</span><span class="text_14">(label_encoder[-</span><span class="text_17">1</span><span class="text_14">].inverse_transform(predict_classifier)[</span><span class="text_17">0</span><span class="text_14">])</span></p><p class="block_45"></p></div>
	<p class="block_8">Kết quả in ra ở Terminal</p>
	<div class="frame_1"><p class="block_20">Đẻ lần F1: 73.23%</p><p class="block_22">&lt;=50K</p></div>
	<p class="block_8">Nếu bạn kiểm tra dữ liệu đó, bạn sẽ thấy nó gần giống với dữ liệu đã cho ở lớp thấp hơn 50k. Bạn có thể thay đổi kết quả của phép phân loại ( F1 score, độ chính xác,…) bằng cách xử dụng nhiều nhân khác nhau và thử kết hợp nhiều biến số.</p>
	<p class="block_7">Hồi quy là gì?</p>
	<p class="block_8">Hồi quy là quá trình xử lý dự đoán,ước tính mối quan hệ giữa biến đầu vào và biến đầu ra. Một thứ cần chú ý ở đây đó là biến đầu ra là biến số liên tục ( continuous-valued). Vì thế nên nó là số vô tận khả năng. Cái này trái ngược với phân loại. nơi các số ở lớp đầu ra là cố định. Các lớp thuộc về một tập có “hữu hạn” khả năng.</p>
	<p class="block_8">Trong hồi quy, giả định rằng các biến đầu ra phụ thuộc vào các biến đầu vào, và chúng ta muốn xem nó liên quan thế nào. Vậy thì giá trị đầu vào được gọi là biến độc lập và được gọi là biến “dự đoán”, còn biến đầu ra được gọi là biến phụ thuộc hay còn gọi là biến tiêu chí. Không phải tất cả các biến đầu vào đều độc lập với nhau. Có rất nhiều trường hợp có mối quan hệ giữa các biến đầu vào với nhau.</p>
	<p class="block_8">Phân tích hồi quy giúp chúng ta hiểu làm thế nào giá trị đầu ra biến đổi khi chúng ta thay đổi một vài biến đầu vào trong khi các biến khác giữ nguyên. Trong hồi quy tuyến tính, chúng ta giả định rằng mối quan hệ giữa các biến đấu vào và đầu ra là tuyến tính. Điều này đặt một mối ràng buộc vào quy trình mô hình ( modeling procedure) của chúng ta, nhưng nó nhanh và hiệu quả. </p>
	<p class="block_8">Đôi khi hồi quy tuyến tính không đủ để phân tích mối quan hệ giữa đầu vào và đầu ra, vì vậy chúng ta sử dụng hồi quy đa thức( polynomial regression). Điều này phức tạp hơn về mặt tính toán nhưng nó cho chúng ta kết quả chính xác hơn. Dựa trên những vấn đề xảy ra với chúng ta, mà chúng ta xử dụng các dạng hồi quy để phân tích mối quan hệ. Hồi quy thường được xử dụng để dự đoán mức giá, kinh tế, sự biến đổi và những thứ tương tự thế.</p>
	<p class="block_8">ví dụ :</p>
	<p class="block_8">1: Ngân hàng KMNO muốn tang lượng tiền huy động vốn. Ngân hàng này muốn biết mối quan hệ giữa lượng tiền gửi và lãi suất tiền gửi. Cụ thể hơn là họ muốn biết khi tăng lãi suất them 0.1% thì lượng tiền gửi sẽ tăng trung bình là bao nhiêu.</p>
	<p class="block_8">2: Một kỹ sư nông nghiệp muốn biết năng suất nuôi tôm sú trong hệ thống thâm canh phụ thuộc thế nào vào diện tích ao nuôi, mật độ thả tôm giống, chi phí hóa chất xử lý môi trường, trình độ nhân công….. Từ phân tích hồi quy ông ta đề ra các chỉ tiêu kỹ thuật phù hợp cho loại hình này.</p>
	<p class="block_7">Xây dựng một biến hồi quy đơn </p>
	<p class="block_8">Đây là một cách xây dựng một mẫu biến hồi quy đơn. Tạo file python với regression_linear.py</p>
	<div class="frame_"><p class="block_11"># import các package cần thiết.</p><p class="block_17">import numpy as np</p><p class="block_36"></p><p class="block_17">import pickle</p><p class="block_17">from sklearn import linear_model</p><p class="block_17">import sklearn.metrics as sm</p><p class="block_17">import matplotlib.pyplot as plt</p><p class="block_17"># Chúng ta xử dụng dữ liệu trong file data_singlevar_regr.txt. Đây là dữ liệu nguồn của chúng ta</p><p class="block_17">input_file=”data/data_singlevar_regr.txt”</p><p class="block_17"># file dữ liệu được ngăn cách bởi dấu , ở mỗi dòng nên chúng ta load và parse data</p><p class="block_17">data=np.loadtxt(input_file,delimiter=’,’)</p><p class="block_17">X,y=data[:,:-1],data[:,-1]</p><p class="block_17"># chia dữ liệu (split data) để training và testing:</p><p class="block_17">num_training=int(0.8*len(X))</p><p class="block_17">num_text=len(X)-num_training</p><p class="block_17"># Training data</p><p class="block_17">X_train,y_train=X[:num_training],y[:num_training]</p><p class="block_17">X_test,y_test=X[num_training:],y[num_training:]</p><p class="block_17">#Tạo một object hồi quy tuyến tính và train nó xử dụng training data:</p><p class="block_17">regressor=linear_model.LinearRegression()</p><p class="block_17">regressor.fit(X_train,y_train)</p><p class="block_17">#dự đoán kết quả đầu ra cho dữ liệu kiểm tra xử dụng training model:</p><p class="block_17">y_test_pred=regressor.predict(X_test)</p><p class="block_17"># Vẽ đồ thị kết quả đầu ra xử dụng plot(plt)</p><p class="block_17">plt.scatter(X_test,y_test,color=’green’)</p><p class="block_17">plt.plot(X_test,y_test_pred,color=’black’,linewidth=4)</p><p class="block_17">plt.xticks(())</p><p class="block_17">plt.yticks(())</p><p class="block_17">plt.show()</p><p class="block_17"># Tính toán số liệu hiệu suất cho object hồi quy (regressor bằng cách so sánh với đầu ra thực sự (ground truth, cùng với đầu ra đã dự đoán (predicted outputs):</p><p class="block_17"># In tính toán số liệu hiệu suất</p><p class="block_17">print("Linear regressor performance:")</p><p class="block_17">print("Mean absolute error =", round(sm.mean_absolute_error(y_test,y_test_pred), 2))</p><p class="block_17">print("Mean squared error =", round(sm.mean_squared_error(y_test,y_test_pred), 2))</p><p class="block_17">print("Median absolute error =", round(sm.median_absolute_error(y_test,y_test_pred), 2))</p><p class="block_17">print("Explain variance score =", round(sm.explained_variance_score(y_test,y_test_pred), 2))</p><p class="block_17">print("R2 score =", round(sm.r2_score(y_test, y_test_pred), 2))</p><p class="block_17"># Khi mô hình đã được tạo chúng ta có thể lưu nó lại dưới dạng 1 file và dùng lại sau. Python có một công cụ gọi là pickle để chúng ta làm việc đó:</p><p class="block_17">output_model_file=’model.pkl’</p><p class="block_17">with open(output_model_file,’wb’) as f:</p><p class="block_31">pickle.dump(regressor,f)</p><p class="block_17"># giờ thì load model từ file và biểu diễn dự đóan:</p><p class="block_17">with open(output_model_file,’rb’) as f:</p><p class="block_31">regressor_model=pickle.load(f)</p><p class="block_17">y_test_pred_new=regressor_model.predict(X_test)</p><p class="block_17">print(“\n New mean absolute error=”, round(sm.mean_absolute_error(y_test,y_test_pred_new),2)</p><p class="block_45"></p></div>
	<p class="block_8">Chạy code trên ta có hình sau được in ra</p>
	<p class="block_8"><img alt="Image" src="images/000005.png" class="calibre13" /></p>
	<p class="block_8">Và đoạn sau ở Terminal:</p>
	<div class="frame_1"><p class="block_20">Linear regressor performance:</p><p class="block_21">Mean absolute error = 0.59</p><p class="block_21">Mean squared error = 0.49</p><p class="block_21">Median absolute error = 0.51</p><p class="block_21">Explain variance score = 0.86</p><p class="block_21">R2 score = 0.86</p><p class="block_43"></p><p class="block_21"> New mean absolute error= 0.59</p><p class="block_43"></p><p class="block_22">Process finished with exit code 0</p></div>
	<p class="block_13">&nbsp;</p>
	<p class="block_46"><b class="calibre2">Xây dựng một hồi quy đa biến (</b><i class="calibre5"> multivariable regressor </i><b class="calibre2">)</b></p>
	<p class="block_8">Trong phần trên chúng ta đã biết cách làm thế nào để xây dựng một model hồi quy cho biến đơn. Trong phần này chúng ta sẽ làm việc cùng với dữ liệu nhiều chiều ( multidimensional data). Tạo file Python và làm việc nào :</p>
	<p class="block_47"><span class="text_15"># import package<br class="calibre8" /></span><span class="text_13">import </span><span class="text_14">numpy </span><span class="text_13">as </span><span class="text_14">np<br class="calibre8" /></span><span class="text_13">from </span><span class="text_14">sklearn </span><span class="text_13">import </span><span class="text_14">linear_model<br class="calibre8" /></span><span class="text_13">import </span><span class="text_14">matplotlib.pyplot </span><span class="text_13">as </span><span class="text_14">plt<br class="calibre8" /></span><span class="text_13">import </span><span class="text_14">sklearn.metrics </span><span class="text_13">as </span><span class="text_14">sm<br class="calibre8" /></span><span class="text_13">from </span><span class="text_14">sklearn.preprocessing </span><span class="text_13">import </span><span class="text_14">PolynomialFeatures<br class="calibre8" /></span><span class="text_15"># chúng ta xử dụng file data_multivar_regr.txt để làm dữ liệu<br class="calibre8" /></span><span class="text_14">input_file =</span><span class="text_16">'data/data_multivar_regr.txt'<br class="calibre8" /></span><span class="text_14">data=np.loadtxt(input_file</span><span class="text_13">,</span><span class="text_19">delimiter</span><span class="text_14">=</span><span class="text_16">','</span><span class="text_14">)<br class="calibre8" />X</span><span class="text_13">,</span><span class="text_14">y=data[:</span><span class="text_13">,</span><span class="text_14">:-</span><span class="text_17">1</span><span class="text_14">]</span><span class="text_13">,</span><span class="text_14">data[:</span><span class="text_13">,</span><span class="text_14">-</span><span class="text_17">1</span><span class="text_14">]<br class="calibre8" /></span><span class="text_15"># chia data để training và test<br class="calibre8" /></span><span class="text_14">num_training=</span><span class="text_18">int</span><span class="text_14">(</span><span class="text_17">0.8</span><span class="text_14">*</span><span class="text_18">len</span><span class="text_14">(X))<br class="calibre8" />num_test=</span><span class="text_18">len</span><span class="text_14">(X)-num_training<br class="calibre8" />X_train</span><span class="text_13">,</span><span class="text_14">y_train=X[:num_training]</span><span class="text_13">,</span><span class="text_14">y[:num_training]<br class="calibre8" />X_test</span><span class="text_13">,</span><span class="text_14">y_test=X[num_training:]</span><span class="text_13">,</span><span class="text_14">y[num_training:]<br class="calibre8" /></span><span class="text_15"># tạo linear regressor model:<br class="calibre8" /></span><span class="text_14">linear_regressor=linear_model.LinearRegression()<br class="calibre8" />linear_regressor.fit(X_train</span><span class="text_13">,</span><span class="text_14">y_train)<br class="calibre8" />y_test_pred=linear_regressor.predict(X_test)<br class="calibre8" /></span><span class="text_18">print</span><span class="text_14">(X_test[-</span><span class="text_17">1</span><span class="text_14">]</span><span class="text_13">,</span><span class="text_14">y_test[-</span><span class="text_17">1</span><span class="text_14">])<br class="calibre8" /></span><span class="text_18">print</span><span class="text_14">(</span><span class="text_16">"Linear Regressor performance:"</span><span class="text_14">)<br class="calibre8" /></span><span class="text_18">print</span><span class="text_14">(</span><span class="text_16">"Mean absolute error ="</span><span class="text_13">, </span><span class="text_18">round</span><span class="text_14">(sm.mean_absolute_error(y_test</span><span class="text_13">,</span><span class="text_14">y_test_pred)</span><span class="text_13">, </span><span class="text_17">2</span><span class="text_14">))<br class="calibre8" /></span><span class="text_18">print</span><span class="text_14">(</span><span class="text_16">"Mean squared error ="</span><span class="text_13">, </span><span class="text_18">round</span><span class="text_14">(sm.mean_squared_error(y_test</span><span class="text_13">,</span><span class="text_14">y_test_pred)</span><span class="text_13">, </span><span class="text_17">2</span><span class="text_14">))<br class="calibre8" /></span><span class="text_18">print</span><span class="text_14">(</span><span class="text_16">"Median absolute error ="</span><span class="text_13">, </span><span class="text_18">round</span><span class="text_14">(sm.median_absolute_error(y_test</span><span class="text_13">,</span><span class="text_14">y_test_pred)</span><span class="text_13">, </span><span class="text_17">2</span><span class="text_14">))<br class="calibre8" /></span><span class="text_18">print</span><span class="text_14">(</span><span class="text_16">"Explained variance score ="</span><span class="text_13">,</span><span class="text_18">round</span><span class="text_14">(sm.explained_variance_score(y_test</span><span class="text_13">, </span><span class="text_14">y_test_pred)</span><span class="text_13">, </span><span class="text_17">2</span><span class="text_14">))<br class="calibre8" /></span><span class="text_18">print</span><span class="text_14">(</span><span class="text_16">"R2 score ="</span><span class="text_13">, </span><span class="text_18">round</span><span class="text_14">(sm.r2_score(y_test</span><span class="text_13">, </span><span class="text_14">y_test_pred)</span><span class="text_13">, </span><span class="text_17">2</span><span class="text_14">))<br class="calibre8" /></span><span class="text_15"><span class="calibre4">#Tạo một đa thức hồi quy(polynomial regression)  bậc 10. Train Hồi quy object này trên training dataset. Lấy một mẫu thử và xem nó thực hiện dự đoán thế nào. Bước đầu tiên là phải chuyển đổi nó thành một đa thức (polynomial):</span></span><span class="text_15"><br class="calibre8" /></span><span class="text_14">polynomial=PolynomialFeatures(</span><span class="text_19">degree</span><span class="text_14">=</span><span class="text_17">10</span><span class="text_14">)<br class="calibre8" />X_train_transformed=polynomial.fit_transform(X_train)<br class="calibre8" />datapoint=[[</span><span class="text_17">7.75</span><span class="text_13">,</span><span class="text_17">6.35</span><span class="text_13">,</span><span class="text_17">5.56</span><span class="text_14">]]<br class="calibre8" />poly_datapoint=polynomial.fit_transform(datapoint)<br class="calibre8" /></span><span class="text_15"># Bạn hãy để ý datapoint này khá giống với datapoint ở dòng 11 trong file txt dữ liệu [7.66,6.29,5.66]. Vậy thì phép hồi quy tốt sẽ dự đoán một kết quả đầu ra gần với số 41.35, Tạo một phép hồi quy tuyến tính (linear regressor) và biểu diễn đa thức .<span class="calibre4"> Biểu diễn dự đoán xử dụng cả  hồi quy tuyến tính và hồi quy đa thức (linear and polynomial regressor) để xem sự khác biệt</span></span><span class="text_15"><br class="calibre8" /></span><span class="text_14">poly_linear_model=linear_model.LinearRegression()<br class="calibre8" />poly_linear_model.fit(X_train_transformed</span><span class="text_13">,</span><span class="text_14">y_train)<br class="calibre8" /></span><span class="text_18">print</span><span class="text_14">(</span><span class="text_16">"</span><span class="text_13">\n</span><span class="text_16"> Linear regression:</span><span class="text_13">\n</span><span class="text_16">"</span><span class="text_13">, </span><span class="text_14">linear_regressor.predict(datapoint))<br class="calibre8" /></span><span class="text_18">print</span><span class="text_14">(</span><span class="text_16">"</span><span class="text_13">\n</span><span class="text_16"> Polynomial regression:</span><span class="text_13">\n</span><span class="text_16">"</span><span class="text_13">,</span><span class="text_14">poly_linear_model.predict(poly_datapoint))<br class="calibre8" />y_linear_pred=linear_regressor.predict(datapoint)<br class="calibre8" />y_poly_pred=poly_linear_model.predict(poly_datapoint)</span></p>
	<p class="block_13">&nbsp;</p>
	<p class="block_46"><b class="calibre2">Dự đoán giá nhà sử dụng SVR</b><i class="calibre5"> ( Support Vector Regressor )</i></p>
	<p class="block_8">Hãy xem code về cách sử dụng khái niệm SVM ( support vectors machine) để dự đoán giá nhà. Chúng ta sẽ sử dụng dataset có trong sklearn nơi các data point được định nghĩa bởi 13 thuộc tính. Nhiệm vụ của chúng ta là dự đoán giá nhà dựa trên các thuộc tính đó.</p>
	<p class="block_47"><span class="text_15">#import package<br class="calibre8" /></span><span class="text_13">from </span><span class="text_14">sklearn </span><span class="text_13">import </span><span class="text_14">datasets<br class="calibre8" /></span><span class="text_13">from </span><span class="text_14">sklearn.svm </span><span class="text_13">import </span><span class="text_14">SVR<br class="calibre8" /></span><span class="text_13">from </span><span class="text_14">sklearn.metrics </span><span class="text_13">import </span><span class="text_14">mean_squared_error</span><span class="text_13">,</span><span class="text_14">explained_variance_score<br class="calibre8" /></span><span class="text_13">from </span><span class="text_14">sklearn.utils </span><span class="text_13">import </span><span class="text_14">shuffle<br class="calibre8" /><br class="calibre8" /></span><span class="text_15"># load dữ liệu từ datasets<br class="calibre8" /></span><span class="text_14">data=datasets.load_boston()<br class="calibre8" /></span><span class="text_15"># thử xáo trộn data để chúng ta không nhầm lẫn dữ liệu phân tích:<br class="calibre8" /></span><span class="text_14">X</span><span class="text_13">,</span><span class="text_14">y=shuffle(data.data</span><span class="text_13">,</span><span class="text_14">data.target</span><span class="text_13">,</span><span class="text_19">random_state</span><span class="text_14">=</span><span class="text_17">7</span><span class="text_14">)<br class="calibre8" /></span><span class="text_15"># Chia dataset để train và test<br class="calibre8" /></span><span class="text_14">num_training = </span><span class="text_18">int</span><span class="text_14">(</span><span class="text_17">0.8</span><span class="text_14">*</span><span class="text_18">len</span><span class="text_14">(X))<br class="calibre8" />X_train</span><span class="text_13">,</span><span class="text_14">y_train=X[:num_training]</span><span class="text_13">,</span><span class="text_14">y[:num_training]<br class="calibre8" />X_test</span><span class="text_13">,</span><span class="text_14">y_test=X[num_training:]</span><span class="text_13">,</span><span class="text_14">y[num_training:]<br class="calibre8" /></span><span class="text_15"># Tạo và train SVR sử dụng một nhân tuyến tính (linear kernel). Tham số C dùng để thay thế/phạt cho dữ liệu train bị lỗi. nếu bạn tăng giá trị của C, model sẽ tốt hơn và trùng hơn với dữ liệu training. Nhưng nó cũng có thể làm Overfitting(quá mức phù hợp - trong Supervised Learning chúng ta chỉ cần xấp xỉ) và sẽ mất tính tổng quát. Tham số epsilon được coi như là một ngưỡng ( threshold); nó không bỏ qua cho dữ liệu train lỗi nếu giá trị dự đoán trong khoảng này so với giá trị thực tế:<br class="calibre8" /></span><span class="text_14">sv_regressor=SVR(</span><span class="text_19">kernel</span><span class="text_14">=</span><span class="text_16">'linear'</span><span class="text_13">,</span><span class="text_19">C</span><span class="text_14">=</span><span class="text_17">1.0</span><span class="text_13">,</span><span class="text_19">epsilon</span><span class="text_14">=</span><span class="text_17">0.1</span><span class="text_14">)<br class="calibre8" />sv_regressor.fit(X_train</span><span class="text_13">,</span><span class="text_14">y_train)<br class="calibre8" /></span><span class="text_15"># Đánh giá hiệu suất của hồi quy và in các số liệu<br class="calibre8" /></span><span class="text_14">y_test_pred=sv_regressor.predict(X_test)<br class="calibre8" />mse=mean_squared_error(y_test</span><span class="text_13">,</span><span class="text_14">y_test_pred)<br class="calibre8" />evs=explained_variance_score(y_test</span><span class="text_13">,</span><span class="text_14">y_test_pred)<br class="calibre8" /></span><span class="text_18">print</span><span class="text_14">(</span><span class="text_16">"</span><span class="text_13">\n</span><span class="text_16"> Performance"</span><span class="text_14">)<br class="calibre8" /></span><span class="text_15"># sai số toàn phương trung bình MSE của một phép dự đoán là trung bình của bình phương các sai số, tức là sự khác biệt giữa các dự đoán và những gì được đánh giá. MSE là hàm rủi ro<br class="calibre8" /></span><span class="text_18">print</span><span class="text_14">(</span><span class="text_16">"Sai số toàn phương trung bình ="</span><span class="text_13">,</span><span class="text_18">round</span><span class="text_14">(mse</span><span class="text_13">,</span><span class="text_17">2</span><span class="text_14">))<br class="calibre8" /></span><span class="text_15"># Điểm phương sai<br class="calibre8" /></span><span class="text_18">print</span><span class="text_14">(</span><span class="text_16">"Điểm phương sai ="</span><span class="text_13">, </span><span class="text_18">round</span><span class="text_14">(evs</span><span class="text_13">,</span><span class="text_17">2</span><span class="text_14">))<br class="calibre8" /></span><span class="text_15"># Lấy một data để kiểm thử và lấy kết quả dự đoán<br class="calibre8" /></span><span class="text_14">test_data=[</span><span class="text_17">3.7</span><span class="text_13">,</span><span class="text_17">0</span><span class="text_13">,</span><span class="text_17">18.4</span><span class="text_13">,</span><span class="text_17">1</span><span class="text_13">,</span><span class="text_17">0.87</span><span class="text_13">,</span><span class="text_17">5.95</span><span class="text_13">,</span><span class="text_17">91</span><span class="text_13">,</span><span class="text_17">2.5052</span><span class="text_13">,</span><span class="text_17">26</span><span class="text_13">,</span><span class="text_17">666</span><span class="text_13">,</span><span class="text_17">20.2</span><span class="text_13">,</span><span class="text_17">351.34</span><span class="text_13">,</span><span class="text_17">15.27</span><span class="text_14">]<br class="calibre8" /></span><span class="text_18">print</span><span class="text_14">(</span><span class="text_16">"</span><span class="text_13">\n</span><span class="text_16"> Dự đoán giá nhà:"</span><span class="text_13">,</span><span class="text_14">sv_regressor.predict([test_data])[</span><span class="text_17">0</span><span class="text_14">])</span></p>
	<p class="block_13">&nbsp;</p>
	<p class="block_8">Chạy file trên với PyCharm ta có kết quả ở Terminal như sau:</p>
	<div class="frame_1"><p class="block_20">D:/PycharmProjects/AILearn/house_prices_SVR.py</p><p class="block_21">Performance</p><p class="block_21">Sai số toàn phương trung bình = 15.38</p><p class="block_21">Điểm phương sai = 0.82</p><p class="block_21">Dự đoán giá nhà: 18.521780107258536</p><p class="block_22">Process finished with exit code 0</p></div>
	<p class="block_7">Tổng Kết</p>
	<p class="block_8">Trong chương này chúng ta học được về sự khác nhau giữa học giám sát và học không giám sát. Chúng ta đã thảo luận các vấn đề vè phân loại data và làm thế nào để giải quyết nó. Chúng ta hiểu làm thế nào để xử lý dữ liệu sử dụng nhiều phương thức khác nhau. Chúng ta cũng học được về mã hóa nhãn và cách nào để xây dựng một class để mã hóa nhãn. Chúng ta thảo luận về lý thuyết phân hồi quy (logistic regression). Chúng ta cũng đã hiểu cái gì là phân loại Naïve Bayes và học làm cách nào để xây dựng lên nó. Chúng ta cũng học về cách xây dựng lên một ma trận hỗn loạn confusion matrix.</p>
	<p class="block_8">Chúng ta đã thảo luận về SVM (Support Vector Machine) và hiểu cách xây dựng một bộ phân loại dựa trên đó. Chúng ta đã học về hồi quy (Regression) và hiểu cách sử dụng hồi quy tuyến tính và hồi quy đa thức ( linear and polynomial ) cho dữ liệu đơn biến và đa biến. Sau đó sử dụng SVR (Support Vector Regressor) để dự đoán giá nhà dựa trên các tham số đã cho.</p>
	<p class="block_8">Trong chapter tiếp theo chúng ta sẽ học về phân tích dự đoán và cách xây dựng một engine dự đoán dựa trên<b class="calibre2"> ensemble learning.</b> </p>
	<p class="block_13">&nbsp;</p>
	<p class="block_13">&nbsp;</p>
	<p class="block_13">&nbsp;</p>
	<p class="block_13">&nbsp;</p>
	<p class="block_13">&nbsp;</p>

</div>

</body></html>

<footer class=" footline" >
	
</footer>


        
        </div> 
        

      </div>

    <div id="navigation">
        
        
        
        
            
            
                
                    
                    
                
                

                    
                    
                        
                    
                    

                    
                        
            
            
                
                    
                    
                
                

                    
                    
                    

                    
                        
            
            
                
                    
                        
                        
                    
                
                

                    
                    
                    

                    
            
        
                    
                        
            
            
                
                    
                    
                
                

                    
                    
                    

                    
            
        
                    
            
        
                    
                        
            
            
                
                    
                        
                        
                    
                
                

                    
                    
                    

                    
            
        
                    
            
        
        
        


	 
	 
		
			<a class="nav nav-prev" href="/ai-with-python/chapter1/" title="Chapter 1"> <i class="fa fa-chevron-left"></i></a>
		
		
			<a class="nav nav-next" href="/about/" title="about" style="margin-right: 0px;"><i class="fa fa-chevron-right"></i></a>
		
	
    </div>

    </section>
    
    <div style="left: -1000px; overflow: scroll; position: absolute; top: -1000px; border: none; box-sizing: content-box; height: 200px; margin: 0px; padding: 0px; width: 200px;">
      <div style="border: none; box-sizing: content-box; height: 200px; margin: 0px; padding: 0px; width: 200px;"></div>
    </div>
    <script src="/js/clipboard.min.js?1568110658"></script>
    <script src="/js/perfect-scrollbar.min.js?1568110658"></script>
    <script src="/js/perfect-scrollbar.jquery.min.js?1568110658"></script>
    <script src="/js/jquery.sticky.js?1568110658"></script>
    <script src="/js/featherlight.min.js?1568110658"></script>
    <script src="/js/highlight.pack.js?1568110658"></script>
    <script>hljs.initHighlightingOnLoad();</script>
    <script src="/js/modernizr.custom-3.6.0.js?1568110658"></script>
    <script src="/js/learn.js?1568110658"></script>
    <script src="/js/hugo-learn.js?1568110658"></script>

    <link href="/mermaid/mermaid.css?1568110658" rel="stylesheet" />
    <script src="/mermaid/mermaid.js?1568110658"></script>
    <script>
        mermaid.initialize({ startOnLoad: true });
    </script>
    

  </body>
</html>

